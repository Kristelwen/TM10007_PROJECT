{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "brat_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ySDv5ko54ViJ",
        "gY9t_GtJ1KrG",
        "tzrJpQt-PM9u",
        "wO_MGCSW8d0l",
        "FqLl7RNggs9n",
        "gL04AA6EH9_7",
        "fuluNEEqjEoY"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kristelwen/TM10007_PROJECT/blob/master/brat_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment, data set 1\n",
        "Kristel Wenink (4450396), Lalot Gunneman (4532155), Marijn van der Graaf (4468708), Tahisa Robles (4531434)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CiDn2Sk-VWqE",
        "outputId": "6e06dc9f-0a0f-4f51-feb1-383a27e23f83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/Kristelwen/TM10007_PROJECT"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for brats (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySDv5ko54ViJ",
        "colab_type": "text"
      },
      "source": [
        "# Data loading and importing modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_Lae4Zh5V3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing modules\n",
        "# General packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Preprocessing packages\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Classifiers\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# SVM Kernels\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.metrics.pairwise import rbf_kernel, sigmoid_kernel\n",
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "# Model selection\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "# Performance metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-NE_fTbKGe5z",
        "outputId": "53facf84-595e-4da9-b4cb-8d9537ece2bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Loading the data\n",
        "from brats.load_data import load_data\n",
        "\n",
        "data = load_data()\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of samples: 167\n",
            "The number of columns: 725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50S8Z8Y968kr",
        "colab_type": "code",
        "outputId": "4e36ea4c-994a-43f2-dba1-4502ad489226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "source": [
        "# Display data for analysis\n",
        "data_top = data.head()\n",
        "data_top "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VOLUME_ET</th>\n",
              "      <th>VOLUME_NET</th>\n",
              "      <th>VOLUME_ED</th>\n",
              "      <th>VOLUME_TC</th>\n",
              "      <th>VOLUME_WT</th>\n",
              "      <th>VOLUME_BRAIN</th>\n",
              "      <th>VOLUME_ET_OVER_NET</th>\n",
              "      <th>VOLUME_ET_OVER_ED</th>\n",
              "      <th>VOLUME_NET_OVER_ED</th>\n",
              "      <th>VOLUME_ET_over_TC</th>\n",
              "      <th>VOLUME_NET_over_TC</th>\n",
              "      <th>VOLUME_ED_over_TC</th>\n",
              "      <th>VOLUME_ET_OVER_WT</th>\n",
              "      <th>VOLUME_NET_OVER_WT</th>\n",
              "      <th>VOLUME_ED_OVER_WT</th>\n",
              "      <th>VOLUME_TC_OVER_WT</th>\n",
              "      <th>VOLUME_ET_OVER_BRAIN</th>\n",
              "      <th>VOLUME_NET_OVER_BRAIN</th>\n",
              "      <th>VOLUME_ED_over_BRAIN</th>\n",
              "      <th>VOLUME_TC_over_BRAIN</th>\n",
              "      <th>VOLUME_WT_OVER_BRAIN</th>\n",
              "      <th>DIST_Vent_TC</th>\n",
              "      <th>DIST_Vent_ED</th>\n",
              "      <th>INTENSITY_Mean_ET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_ET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_ET_T1</th>\n",
              "      <th>INTENSITY_STD_ET_T1</th>\n",
              "      <th>INTENSITY_Mean_ET_T2</th>\n",
              "      <th>INTENSITY_STD_ET_T2</th>\n",
              "      <th>INTENSITY_Mean_ET_FLAIR</th>\n",
              "      <th>INTENSITY_STD_ET_FLAIR</th>\n",
              "      <th>INTENSITY_Mean_NET_T1Gd</th>\n",
              "      <th>INTENSITY_STD_NET_T1Gd</th>\n",
              "      <th>INTENSITY_Mean_NET_T1</th>\n",
              "      <th>INTENSITY_STD_NET_T1</th>\n",
              "      <th>INTENSITY_Mean_NET_T2</th>\n",
              "      <th>INTENSITY_STD_NET_T2</th>\n",
              "      <th>INTENSITY_Mean_NET_FLAIR</th>\n",
              "      <th>INTENSITY_STD_NET_FLAIR</th>\n",
              "      <th>INTENSITY_Mean_ED_T1Gd</th>\n",
              "      <th>...</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T1_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_T2_Strength</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Coarseness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Contrast</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Busyness</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Complexity</th>\n",
              "      <th>TEXTURE_NGTDM_NET_FLAIR_Strength</th>\n",
              "      <th>TGM_p1</th>\n",
              "      <th>TGM_dw</th>\n",
              "      <th>TGM_Cog_X_1</th>\n",
              "      <th>TGM_Cog_Y_1</th>\n",
              "      <th>TGM_Cog_Z_1</th>\n",
              "      <th>TGM_T_1</th>\n",
              "      <th>TGM_Cog_X_2</th>\n",
              "      <th>TGM_Cog_Y_2</th>\n",
              "      <th>TGM_Cog_Z_2</th>\n",
              "      <th>TGM_T_2</th>\n",
              "      <th>TGM_Cog_X_3</th>\n",
              "      <th>TGM_Cog_Y_3</th>\n",
              "      <th>TGM_Cog_Z_3</th>\n",
              "      <th>TGM_T_3</th>\n",
              "      <th>TGM_Cog_X_4</th>\n",
              "      <th>TGM_Cog_Y_4</th>\n",
              "      <th>TGM_Cog_Z_4</th>\n",
              "      <th>TGM_T_4</th>\n",
              "      <th>TGM_Cog_X_5</th>\n",
              "      <th>TGM_Cog_Y_5</th>\n",
              "      <th>TGM_Cog_Z_5</th>\n",
              "      <th>TGM_T_5</th>\n",
              "      <th>TGM_Cog_X_6</th>\n",
              "      <th>TGM_Cog_Y_6</th>\n",
              "      <th>TGM_Cog_Z_6</th>\n",
              "      <th>TGM_T_6</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>TCGA-02-0006</th>\n",
              "      <td>1662</td>\n",
              "      <td>384</td>\n",
              "      <td>36268</td>\n",
              "      <td>2046</td>\n",
              "      <td>38314</td>\n",
              "      <td>1469432</td>\n",
              "      <td>4.328125</td>\n",
              "      <td>0.0458255</td>\n",
              "      <td>0.0105878</td>\n",
              "      <td>0.81232</td>\n",
              "      <td>0.18768</td>\n",
              "      <td>17.7263</td>\n",
              "      <td>0.043378</td>\n",
              "      <td>0.010022</td>\n",
              "      <td>0.946599</td>\n",
              "      <td>0.053401</td>\n",
              "      <td>0.001131</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.024682</td>\n",
              "      <td>0.001392</td>\n",
              "      <td>0.026074</td>\n",
              "      <td>31.5903</td>\n",
              "      <td>2.7735</td>\n",
              "      <td>149.7977</td>\n",
              "      <td>10.4671</td>\n",
              "      <td>194.1422</td>\n",
              "      <td>15.1037</td>\n",
              "      <td>154.9225</td>\n",
              "      <td>43.4709</td>\n",
              "      <td>220.5894</td>\n",
              "      <td>30.2917</td>\n",
              "      <td>137.8881</td>\n",
              "      <td>6.3820</td>\n",
              "      <td>183.6933</td>\n",
              "      <td>14.8846</td>\n",
              "      <td>161.1005</td>\n",
              "      <td>35.8591</td>\n",
              "      <td>227.7510</td>\n",
              "      <td>23.9509</td>\n",
              "      <td>131.8402</td>\n",
              "      <td>...</td>\n",
              "      <td>0.02674</td>\n",
              "      <td>2536.7559</td>\n",
              "      <td>43.3129</td>\n",
              "      <td>0.036634</td>\n",
              "      <td>0.50304</td>\n",
              "      <td>0.024264</td>\n",
              "      <td>3593.3279</td>\n",
              "      <td>43.67590</td>\n",
              "      <td>0.057204</td>\n",
              "      <td>0.33980</td>\n",
              "      <td>0.021897</td>\n",
              "      <td>2203.2034</td>\n",
              "      <td>61.3293</td>\n",
              "      <td>8.00000</td>\n",
              "      <td>7.500000e-07</td>\n",
              "      <td>0.178609</td>\n",
              "      <td>0.096256</td>\n",
              "      <td>0.052741</td>\n",
              "      <td>2.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TCGA-02-0009</th>\n",
              "      <td>4362</td>\n",
              "      <td>4349</td>\n",
              "      <td>15723</td>\n",
              "      <td>8711</td>\n",
              "      <td>24434</td>\n",
              "      <td>1295721</td>\n",
              "      <td>1.002989</td>\n",
              "      <td>0.277428</td>\n",
              "      <td>0.276601</td>\n",
              "      <td>0.50075</td>\n",
              "      <td>0.49925</td>\n",
              "      <td>1.8050</td>\n",
              "      <td>0.178522</td>\n",
              "      <td>0.177990</td>\n",
              "      <td>0.643489</td>\n",
              "      <td>0.356511</td>\n",
              "      <td>0.003366</td>\n",
              "      <td>0.003356</td>\n",
              "      <td>0.012135</td>\n",
              "      <td>0.006723</td>\n",
              "      <td>0.018857</td>\n",
              "      <td>9.2443</td>\n",
              "      <td>3.0207</td>\n",
              "      <td>165.4345</td>\n",
              "      <td>6.4047</td>\n",
              "      <td>201.2400</td>\n",
              "      <td>13.4733</td>\n",
              "      <td>113.1601</td>\n",
              "      <td>10.1373</td>\n",
              "      <td>210.1810</td>\n",
              "      <td>15.9543</td>\n",
              "      <td>152.6013</td>\n",
              "      <td>4.2360</td>\n",
              "      <td>188.0607</td>\n",
              "      <td>11.1316</td>\n",
              "      <td>116.8538</td>\n",
              "      <td>10.0992</td>\n",
              "      <td>209.7901</td>\n",
              "      <td>16.7943</td>\n",
              "      <td>139.2815</td>\n",
              "      <td>...</td>\n",
              "      <td>0.31558</td>\n",
              "      <td>967.7845</td>\n",
              "      <td>3.7444</td>\n",
              "      <td>0.003790</td>\n",
              "      <td>0.36163</td>\n",
              "      <td>0.271420</td>\n",
              "      <td>1996.1440</td>\n",
              "      <td>2.77050</td>\n",
              "      <td>0.004966</td>\n",
              "      <td>0.28715</td>\n",
              "      <td>0.189980</td>\n",
              "      <td>1440.4285</td>\n",
              "      <td>3.5999</td>\n",
              "      <td>3.31250</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.077618</td>\n",
              "      <td>0.122900</td>\n",
              "      <td>0.094336</td>\n",
              "      <td>91.4736</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TCGA-02-0011</th>\n",
              "      <td>33404</td>\n",
              "      <td>48612</td>\n",
              "      <td>45798</td>\n",
              "      <td>82016</td>\n",
              "      <td>127814</td>\n",
              "      <td>1425843</td>\n",
              "      <td>0.687155</td>\n",
              "      <td>0.729377</td>\n",
              "      <td>1.06144</td>\n",
              "      <td>0.40729</td>\n",
              "      <td>0.59271</td>\n",
              "      <td>0.5584</td>\n",
              "      <td>0.261349</td>\n",
              "      <td>0.380334</td>\n",
              "      <td>0.358318</td>\n",
              "      <td>0.641682</td>\n",
              "      <td>0.023428</td>\n",
              "      <td>0.034094</td>\n",
              "      <td>0.032120</td>\n",
              "      <td>0.057521</td>\n",
              "      <td>0.089641</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>186.3385</td>\n",
              "      <td>17.6126</td>\n",
              "      <td>188.2019</td>\n",
              "      <td>23.5195</td>\n",
              "      <td>172.8969</td>\n",
              "      <td>32.7401</td>\n",
              "      <td>167.1395</td>\n",
              "      <td>34.1684</td>\n",
              "      <td>149.0643</td>\n",
              "      <td>12.9090</td>\n",
              "      <td>158.4197</td>\n",
              "      <td>15.2632</td>\n",
              "      <td>197.4966</td>\n",
              "      <td>27.1781</td>\n",
              "      <td>165.1014</td>\n",
              "      <td>25.2120</td>\n",
              "      <td>161.5790</td>\n",
              "      <td>...</td>\n",
              "      <td>1.39580</td>\n",
              "      <td>1322.6082</td>\n",
              "      <td>0.7473</td>\n",
              "      <td>0.000634</td>\n",
              "      <td>0.31856</td>\n",
              "      <td>1.144300</td>\n",
              "      <td>2517.8629</td>\n",
              "      <td>0.84294</td>\n",
              "      <td>0.000794</td>\n",
              "      <td>0.17961</td>\n",
              "      <td>1.068800</td>\n",
              "      <td>1147.5177</td>\n",
              "      <td>0.8048</td>\n",
              "      <td>5.78125</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.132283</td>\n",
              "      <td>0.116006</td>\n",
              "      <td>0.096035</td>\n",
              "      <td>272.4290</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TCGA-02-0027</th>\n",
              "      <td>12114</td>\n",
              "      <td>7587</td>\n",
              "      <td>34086</td>\n",
              "      <td>19701</td>\n",
              "      <td>53787</td>\n",
              "      <td>1403429</td>\n",
              "      <td>1.596679</td>\n",
              "      <td>0.355395</td>\n",
              "      <td>0.222584</td>\n",
              "      <td>0.61489</td>\n",
              "      <td>0.38511</td>\n",
              "      <td>1.7302</td>\n",
              "      <td>0.225222</td>\n",
              "      <td>0.141056</td>\n",
              "      <td>0.633722</td>\n",
              "      <td>0.366278</td>\n",
              "      <td>0.008632</td>\n",
              "      <td>0.005406</td>\n",
              "      <td>0.024288</td>\n",
              "      <td>0.014038</td>\n",
              "      <td>0.038325</td>\n",
              "      <td>1.0331</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>178.6925</td>\n",
              "      <td>23.1751</td>\n",
              "      <td>199.7626</td>\n",
              "      <td>27.0047</td>\n",
              "      <td>157.0192</td>\n",
              "      <td>25.6793</td>\n",
              "      <td>173.6525</td>\n",
              "      <td>26.3596</td>\n",
              "      <td>120.3726</td>\n",
              "      <td>17.5926</td>\n",
              "      <td>199.5765</td>\n",
              "      <td>25.3652</td>\n",
              "      <td>194.2708</td>\n",
              "      <td>24.5411</td>\n",
              "      <td>207.5531</td>\n",
              "      <td>32.7868</td>\n",
              "      <td>129.2927</td>\n",
              "      <td>...</td>\n",
              "      <td>0.14356</td>\n",
              "      <td>1722.6804</td>\n",
              "      <td>6.9449</td>\n",
              "      <td>0.002126</td>\n",
              "      <td>0.54383</td>\n",
              "      <td>0.379490</td>\n",
              "      <td>3698.6228</td>\n",
              "      <td>2.31820</td>\n",
              "      <td>0.003284</td>\n",
              "      <td>0.41179</td>\n",
              "      <td>0.206600</td>\n",
              "      <td>3320.1690</td>\n",
              "      <td>4.7336</td>\n",
              "      <td>3.87500</td>\n",
              "      <td>1.000000e-09</td>\n",
              "      <td>0.100415</td>\n",
              "      <td>0.088249</td>\n",
              "      <td>0.096470</td>\n",
              "      <td>128.4680</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TCGA-02-0033</th>\n",
              "      <td>34538</td>\n",
              "      <td>7137</td>\n",
              "      <td>65653</td>\n",
              "      <td>41675</td>\n",
              "      <td>107328</td>\n",
              "      <td>1365237</td>\n",
              "      <td>4.839288</td>\n",
              "      <td>0.526069</td>\n",
              "      <td>0.108708</td>\n",
              "      <td>0.82875</td>\n",
              "      <td>0.17125</td>\n",
              "      <td>1.5754</td>\n",
              "      <td>0.321799</td>\n",
              "      <td>0.066497</td>\n",
              "      <td>0.611704</td>\n",
              "      <td>0.388296</td>\n",
              "      <td>0.025298</td>\n",
              "      <td>0.005228</td>\n",
              "      <td>0.048089</td>\n",
              "      <td>0.030526</td>\n",
              "      <td>0.078615</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>172.4109</td>\n",
              "      <td>27.5731</td>\n",
              "      <td>121.4969</td>\n",
              "      <td>10.3061</td>\n",
              "      <td>148.9331</td>\n",
              "      <td>27.8493</td>\n",
              "      <td>159.0135</td>\n",
              "      <td>23.9666</td>\n",
              "      <td>116.9944</td>\n",
              "      <td>8.2358</td>\n",
              "      <td>117.7009</td>\n",
              "      <td>9.9957</td>\n",
              "      <td>139.4320</td>\n",
              "      <td>34.3293</td>\n",
              "      <td>139.3234</td>\n",
              "      <td>12.9900</td>\n",
              "      <td>120.7850</td>\n",
              "      <td>...</td>\n",
              "      <td>0.71382</td>\n",
              "      <td>538.8446</td>\n",
              "      <td>1.1436</td>\n",
              "      <td>0.002162</td>\n",
              "      <td>0.47817</td>\n",
              "      <td>0.555670</td>\n",
              "      <td>3020.3680</td>\n",
              "      <td>1.90570</td>\n",
              "      <td>0.003108</td>\n",
              "      <td>0.31043</td>\n",
              "      <td>0.413750</td>\n",
              "      <td>1834.1052</td>\n",
              "      <td>2.4532</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>5.725000e-08</td>\n",
              "      <td>0.106184</td>\n",
              "      <td>0.131952</td>\n",
              "      <td>0.096894</td>\n",
              "      <td>240.7780</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>GBM</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 725 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              VOLUME_ET  VOLUME_NET  VOLUME_ED  ...  TGM_Cog_Z_6  TGM_T_6  label\n",
              "ID                                              ...                             \n",
              "TCGA-02-0006       1662         384      36268  ...          NaN      NaN    GBM\n",
              "TCGA-02-0009       4362        4349      15723  ...          NaN      NaN    GBM\n",
              "TCGA-02-0011      33404       48612      45798  ...          NaN      NaN    GBM\n",
              "TCGA-02-0027      12114        7587      34086  ...          NaN      NaN    GBM\n",
              "TCGA-02-0033      34538        7137      65653  ...          NaN      NaN    GBM\n",
              "\n",
              "[5 rows x 725 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY9t_GtJ1KrG",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu6Fcr8N-tlM",
        "colab_type": "code",
        "outputId": "c6f47fa5-fc4a-47a3-a6e3-aafd6b7de58d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Drop columns which contain NaN values\n",
        "threshold = math.floor(len(data)/2)  # calculate threshold, half of total rows\n",
        "data_drop = data.dropna(thresh=threshold, axis=1)  # Delete columns/features with more than 'threshold' NaNs\n",
        "data_drop = data_drop.fillna(data_drop.median())  # Replace the remaining NaNs with median of feature\n",
        "\n",
        "data_drop.isnull().values.any()  # Check if all the NaNs are deleted or replaced (False = there are no NaNs left)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F45dLJCIa7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split data and labels\n",
        "labels = data_drop['label']\n",
        "data_drop = data_drop.drop(columns=\"label\")  # Data without labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl2rwSBvHPjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert labels 'GBM' and 'LGG' to respectively 0 and 1\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Replace strings and infinity values with median of feature\n",
        "data_no_strings = data_drop.replace('#DIV/0!', np.nan)\n",
        "data_no_strings = data_no_strings.replace([np.inf, -np.inf], np.nan)\n",
        "data_no_strings = data_no_strings.fillna(data_no_strings.median())\n",
        "\n",
        "\n",
        "# Split the data in a train (90%) and test set (10%)\n",
        "data_train, data_test, label_train, label_test = train_test_split(data_no_strings, labels, test_size=0.2, stratify=labels)\n",
        "# data_train2, data_val, label_train2, label_val = train_test_split(data_train, label_train, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm3fAHRsHwjo",
        "colab_type": "code",
        "outputId": "acf133f4-431b-4429-aeec-bccafdcbb4a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# Feature scaling\n",
        "scaler = RobustScaler()\n",
        "transformer = scaler.fit(data_train.values)\n",
        "data_scaled_train = transformer.transform(data_train.values)\n",
        "data_df_train = pd.DataFrame(data_scaled_train, index = data_train.index, columns = data_train.columns)\n",
        "\n",
        "data_scaled_test = transformer.transform(data_test.values)\n",
        "data_df_test = pd.DataFrame(data_scaled_test, index = data_test.index, columns = data_test.columns)\n",
        "\n",
        "# PCA\n",
        "pca_plot = PCA().fit(data_df_train)\n",
        "plt.plot(np.cumsum(pca_plot.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance');\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h59J5JDSDpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature selection with PCA\n",
        "  # Training set\n",
        "pca_train = PCA(0.95)  # Create a PCA with 130 components\n",
        "pca_train.fit(data_df_train)  # Fit PCA\n",
        "data_train_pca = pca_train.transform(data_df_train)\n",
        "\n",
        "  # Test set\n",
        "data_test_pca = pca_train.transform(data_df_test)  # Transform test data using PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANFuoo-r_yek",
        "colab_type": "text"
      },
      "source": [
        "# Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iBDajYWG_jR",
        "colab_type": "text"
      },
      "source": [
        "## k-Nearest-Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjD9zPASHD4s",
        "colab_type": "code",
        "outputId": "5627df84-0588-4c64-c39f-23fc96b32729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        }
      },
      "source": [
        "# Using different n_neighbors with weights = 'distance'\n",
        "print('Results when weights are based on distance:')\n",
        "n_neighbors=[1,5,7,10,11,15,20,50,100]\n",
        "for n_neighbor in n_neighbors:\n",
        "  clf = KNeighborsClassifier(n_neighbors=n_neighbor, weights='distance')\n",
        "  clf.fit(data_train_pca, label_train)\n",
        "\n",
        "  pred_train_knn = clf.predict(data_train_pca)\n",
        "  acc_train_knn = accuracy_score(label_train, pred_train_knn)\n",
        "  print ('Train set: {} neighbors gives an accuracy of {}'.format(n_neighbor, round(acc_train_knn,2)))\n",
        "\n",
        "  pred_test_knn = clf.predict(data_test_pca)\n",
        "  acc_test_knn = accuracy_score(label_test, pred_test_knn)\n",
        "  print ('Test set: {} neighbors gives an accuracy of {}'.format(n_neighbor, round(acc_train_knn,2)))\n",
        "\n",
        "# Using different n_neighbors with weights = 'uniform'\n",
        "print('\\n Results when weights are uniform:')\n",
        "n_neighbors=[1,5,7,10,11,15,20,50,100]\n",
        "for n_neighbor in n_neighbors:\n",
        "  clf = KNeighborsClassifier(n_neighbors=n_neighbor, weights='uniform')\n",
        "  clf.fit(data_train_pca, label_train)\n",
        "\n",
        "  pred_train_knn = clf.predict(data_train_pca)\n",
        "  acc_train_knn = accuracy_score(label_train, pred_train_knn)\n",
        "  print ('Train set: {} neighbors gives an accuracy of {}'.format(n_neighbor, round(acc_train_knn,2)))\n",
        "  \n",
        "  pred_test_knn = clf.predict(data_test_pca)\n",
        "  acc_test_knn = accuracy_score(label_test, pred_test_knn)\n",
        "  print ('Test set: {} neighbors gives an accuracy of {}'.format(n_neighbor, round(acc_train_knn,2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results when weights are based on distance:\n",
            "Train set: 1 neighbors gives an accuracy of 1.0\n",
            "Test set: 1 neighbors gives an accuracy of 1.0\n",
            "Train set: 5 neighbors gives an accuracy of 1.0\n",
            "Test set: 5 neighbors gives an accuracy of 1.0\n",
            "Train set: 7 neighbors gives an accuracy of 1.0\n",
            "Test set: 7 neighbors gives an accuracy of 1.0\n",
            "Train set: 10 neighbors gives an accuracy of 1.0\n",
            "Test set: 10 neighbors gives an accuracy of 1.0\n",
            "Train set: 11 neighbors gives an accuracy of 1.0\n",
            "Test set: 11 neighbors gives an accuracy of 1.0\n",
            "Train set: 15 neighbors gives an accuracy of 1.0\n",
            "Test set: 15 neighbors gives an accuracy of 1.0\n",
            "Train set: 20 neighbors gives an accuracy of 1.0\n",
            "Test set: 20 neighbors gives an accuracy of 1.0\n",
            "Train set: 50 neighbors gives an accuracy of 1.0\n",
            "Test set: 50 neighbors gives an accuracy of 1.0\n",
            "Train set: 100 neighbors gives an accuracy of 1.0\n",
            "Test set: 100 neighbors gives an accuracy of 1.0\n",
            "\n",
            " Results when weights are uniform:\n",
            "Train set: 1 neighbors gives an accuracy of 1.0\n",
            "Test set: 1 neighbors gives an accuracy of 1.0\n",
            "Train set: 5 neighbors gives an accuracy of 0.74\n",
            "Test set: 5 neighbors gives an accuracy of 0.74\n",
            "Train set: 7 neighbors gives an accuracy of 0.71\n",
            "Test set: 7 neighbors gives an accuracy of 0.71\n",
            "Train set: 10 neighbors gives an accuracy of 0.71\n",
            "Test set: 10 neighbors gives an accuracy of 0.71\n",
            "Train set: 11 neighbors gives an accuracy of 0.7\n",
            "Test set: 11 neighbors gives an accuracy of 0.7\n",
            "Train set: 15 neighbors gives an accuracy of 0.71\n",
            "Test set: 15 neighbors gives an accuracy of 0.71\n",
            "Train set: 20 neighbors gives an accuracy of 0.71\n",
            "Test set: 20 neighbors gives an accuracy of 0.71\n",
            "Train set: 50 neighbors gives an accuracy of 0.66\n",
            "Test set: 50 neighbors gives an accuracy of 0.66\n",
            "Train set: 100 neighbors gives an accuracy of 0.61\n",
            "Test set: 100 neighbors gives an accuracy of 0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDIepeN9JT14",
        "colab_type": "code",
        "outputId": "c1e40bb7-d092-46f2-f43a-f2a32c98c666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Hyperparameter optimization of number of neighbors and weights\n",
        "\n",
        "# Our parameter to optimize is the number of estimators, which we vary uniformly between 1 and 400\n",
        "param_distributions = {'n_neighbors': [1,5,7,10,11,15,20,50,100], 'weights': ['uniform','distance']}  # Willen we dit toevoegen: 'bootstrap': [True, False]? Maar dan komt vgm vaak False eruit, mar die overfit. \n",
        "    \n",
        "# Execute RandomizedSearchCV to find optimal parameters\n",
        "clf_knn = RandomizedSearchCV(KNeighborsClassifier(), param_distributions, cv=5, return_train_score=True)\n",
        "\n",
        "# Fit the classifier\n",
        "clf_knn.fit(data_train_pca, label_train)\n",
        "\n",
        "# The best parameters\n",
        "print(f'The best parameters are {clf_knn.best_params_}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "but the best amount of neighbors is {'weights': 'uniform', 'n_neighbors': 11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHoVAsZwdT8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # The dataframe of the hyperparameter optimization\n",
        "# df = pd.DataFrame(clf_knn.cv_results_)\n",
        "\n",
        "# # Get the best parameters for the Random Forest estimator\n",
        "# try:\n",
        "#   # Method to avoid overfitting\n",
        "#   min_value = 0.70\n",
        "#   mask = df[\"mean_test_score\"] > min_value\n",
        "#   df_high = df[mask]\n",
        "#   max_value = 0.98\n",
        "#   mask2 = df_high[\"mean_train_score\"] < max_value\n",
        "#   df_high = df_high[mask2]\n",
        "#   df_high[\"test-train\"] = abs(df_high[\"mean_test_score\"]-df_high[\"mean_train_score\"])\n",
        "#   index_min = df_high[\"test-train\"].idxmin(df_high[\"test-train\"])\n",
        "#   best_parameters = df_high.loc[index_min, \"params\"]\n",
        "#   print(f'\\n Using {min_value} as the minimal test accuracy and {max_value} as the maximal training accuracy,'\n",
        "#         f'\\n the best parameters are {best_parameters}')\n",
        "#   prevent_overfitting_knn = True\n",
        "\n",
        "# except ValueError:\n",
        "#    # Method if all the training scores are overfit (100%)\n",
        "#   best_parameters = clf_knn.best_params_\n",
        "#   print(f'\\n The classifier is overfit on the training set (train_accuracy > {max_value}) or the test accuracy is always smaller than {min_value}, ' \n",
        "#         f'but the best amount of neighbors is {best_parameters}')\n",
        "#   prevent_overfitting_knn = False\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prh3ZaQoWbkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting the K-NN Classifier\n",
        "KNN = KNeighborsClassifier(n_neighbors=11, weights='uniform')\n",
        "KNN.fit(data_train_pca, label_train)\n",
        "\n",
        "# Predictions\n",
        "train_pred = KNN.predict(data_train_pca)\n",
        "test_pred = KNN.predict(data_test_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m_gEMYgW7eo",
        "colab_type": "code",
        "outputId": "9e7e2d35-b0e8-4f2d-874e-6a56f0969c89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Evaluate KNN using cross-validation\n",
        "score_knn = cross_val_score(KNN, data_train_pca, label_train, cv=5)\n",
        "mean_score_knn = score_knn.mean()\n",
        "\n",
        "print(f'The accuracy of the validation set in 5 different folds is {score_knn}')\n",
        "print(f'\\n The mean accuracy of the validation set of 5 different folds is {round(mean_score_knn,2)}')\n",
        "\n",
        "# Evaluate accuracy of KNN on training set\n",
        "acc_train = accuracy_score(label_train, train_pred)\n",
        "print(f'\\n The accuracy of the training set is {round(acc_train,2)}')\n",
        "\n",
        "# Evaluate accuracy of neural network on test set\n",
        "acc_test = accuracy_score(label_test, test_pred)\n",
        "print(f'\\n The accuracy of the test set is {round(acc_test,2)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of the validation set in 5 different folds is [0.74074074 0.74074074 0.59259259 0.61538462 0.80769231]\n",
            "\n",
            " The mean accuracy of the validation set of 5 different folds is 0.7\n",
            "\n",
            " The accuracy of the training set is 0.7\n",
            "\n",
            " The accuracy of the test set is 0.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JgSmsPxiHHq",
        "colab_type": "code",
        "outputId": "3f885d9d-6c58-4817-9da0-92cedc9f3ed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "source": [
        "# Confusion matrices and classification reports\n",
        "print('Confusion matrix and classification report of training set')\n",
        "print(confusion_matrix(label_train, train_pred))\n",
        "print(classification_report(label_train, train_pred))\n",
        "print(\"---------------------------------------------------------------\\n\")\n",
        "\n",
        "print('Confusion matrix and classification report of test set')\n",
        "print(confusion_matrix(label_test, test_pred))\n",
        "print(classification_report(label_test, test_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix and classification report of training set\n",
            "[[71 10]\n",
            " [30 22]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.88      0.78        81\n",
            "           1       0.69      0.42      0.52        52\n",
            "\n",
            "    accuracy                           0.70       133\n",
            "   macro avg       0.70      0.65      0.65       133\n",
            "weighted avg       0.70      0.70      0.68       133\n",
            "\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Confusion matrix and classification report of test set\n",
            "[[21  0]\n",
            " [ 9  4]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.82        21\n",
            "           1       1.00      0.31      0.47        13\n",
            "\n",
            "    accuracy                           0.74        34\n",
            "   macro avg       0.85      0.65      0.65        34\n",
            "weighted avg       0.81      0.74      0.69        34\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWWd0nzQPEfa",
        "colab_type": "text"
      },
      "source": [
        "## Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA1o4T0F6lOy",
        "colab_type": "code",
        "outputId": "8a5719d9-311d-4fa9-b993-ead11062ce39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        }
      },
      "source": [
        "# Hyperparameter optimization: trying out different amount of trees and compare bootstrapping True and False.\n",
        "\n",
        "#using different n_trees and bootstrapping  \n",
        "print('Results when bootstrapping=True')\n",
        "n_trees=[1,5,10,15,20,30,50,100,150,200]\n",
        "for n_tree in n_trees:\n",
        "  clf = RandomForestClassifier(n_estimators=n_tree, bootstrap=True)\n",
        "  clf.fit(data_train_pca,label_train)\n",
        "  \n",
        "  predictions_train = clf.predict(data_train_pca)\n",
        "  acc_train_RF = accuracy_score(label_train, predictions_train)\n",
        "  \n",
        "  print ('Train set: {} trees gives an accuracy of {}'.format(n_tree, round(acc_train_RF,2)))\n",
        "\n",
        "  predictions_test = clf.predict(data_test_pca)\n",
        "  acc_test_RF = accuracy_score(label_test, predictions_test)\n",
        "\n",
        "  print ('Test set: {} trees gives an accuracy of {}'.format(n_tree, round(acc_test_RF, 2)))\n",
        "\n",
        "print ('')\n",
        "\n",
        "#using different n_trees without bootstrapping \n",
        "print('Results when bootstrapping=False')\n",
        "for n_tree in n_trees:\n",
        "  clf = RandomForestClassifier(n_estimators=n_tree, bootstrap=False)\n",
        "  clf.fit(data_train_pca,label_train)\n",
        "  \n",
        "  predictions_train = clf.predict(data_train_pca)\n",
        "  acc_train_RF = accuracy_score(label_train, predictions_train)\n",
        "  print ('Train set: {} trees gives an accuracy of {}'.format(n_tree,round(acc_train_RF,2)))\n",
        "\n",
        "  predictions_test = clf.predict(data_test_pca)\n",
        "  acc_test_RF = accuracy_score(label_test, predictions_test)\n",
        "\n",
        "  print ('Test set: {} trees gives an accuracy of {}'.format(n_tree,round(acc_test_RF,2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results when bootstrapping=True\n",
            "Train set: 1 trees gives an accuracy of 0.81\n",
            "Test set: 1 trees gives an accuracy of 0.59\n",
            "Train set: 5 trees gives an accuracy of 0.91\n",
            "Test set: 5 trees gives an accuracy of 0.59\n",
            "Train set: 10 trees gives an accuracy of 0.92\n",
            "Test set: 10 trees gives an accuracy of 0.59\n",
            "Train set: 15 trees gives an accuracy of 0.95\n",
            "Test set: 15 trees gives an accuracy of 0.62\n",
            "Train set: 20 trees gives an accuracy of 0.95\n",
            "Test set: 20 trees gives an accuracy of 0.59\n",
            "Train set: 30 trees gives an accuracy of 0.99\n",
            "Test set: 30 trees gives an accuracy of 0.59\n",
            "Train set: 50 trees gives an accuracy of 0.99\n",
            "Test set: 50 trees gives an accuracy of 0.59\n",
            "Train set: 100 trees gives an accuracy of 0.99\n",
            "Test set: 100 trees gives an accuracy of 0.59\n",
            "Train set: 150 trees gives an accuracy of 0.99\n",
            "Test set: 150 trees gives an accuracy of 0.59\n",
            "Train set: 200 trees gives an accuracy of 0.99\n",
            "Test set: 200 trees gives an accuracy of 0.59\n",
            "\n",
            "Results when bootstrapping=False\n",
            "Train set: 1 trees gives an accuracy of 0.99\n",
            "Test set: 1 trees gives an accuracy of 0.59\n",
            "Train set: 5 trees gives an accuracy of 0.99\n",
            "Test set: 5 trees gives an accuracy of 0.59\n",
            "Train set: 10 trees gives an accuracy of 0.99\n",
            "Test set: 10 trees gives an accuracy of 0.59\n",
            "Train set: 15 trees gives an accuracy of 0.99\n",
            "Test set: 15 trees gives an accuracy of 0.59\n",
            "Train set: 20 trees gives an accuracy of 0.99\n",
            "Test set: 20 trees gives an accuracy of 0.59\n",
            "Train set: 30 trees gives an accuracy of 0.99\n",
            "Test set: 30 trees gives an accuracy of 0.59\n",
            "Train set: 50 trees gives an accuracy of 0.99\n",
            "Test set: 50 trees gives an accuracy of 0.59\n",
            "Train set: 100 trees gives an accuracy of 0.99\n",
            "Test set: 100 trees gives an accuracy of 0.59\n",
            "Train set: 150 trees gives an accuracy of 0.99\n",
            "Test set: 150 trees gives an accuracy of 0.59\n",
            "Train set: 200 trees gives an accuracy of 0.99\n",
            "Test set: 200 trees gives an accuracy of 0.59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w27ZGUSg9TPx",
        "colab_type": "code",
        "outputId": "db7e88da-616a-49d1-a9b8-9364f26ed312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Hyperparameter optimization of Random Forest Classifier\n",
        "\n",
        "# Our parameter to optimize is the number of estimators, which we vary uniformly between 1 and 400\n",
        "param_distributions = {'n_estimators': [1,5,10,15,20,30,50,100,150,200]}  # Willen we dit toevoegen: 'bootstrap': [True, False]? Maar dan komt vgm vaak False eruit, mar die overfit. \n",
        "    \n",
        "# Execute RandomizedSearchCV to find optimal parameters\n",
        "clf_rf = RandomizedSearchCV(RandomForestClassifier(bootstrap=True), param_distributions, cv=5, return_train_score=True)\n",
        "\n",
        "# Fit the classifier\n",
        "clf_rf.fit(data_train_pca, label_train)\n",
        "\n",
        "# The best parameters\n",
        "print(f'The best parameters are {clf_rf.best_params_}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best parameters are {'n_estimators': 20}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUuehj-7E0HU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # The dataframe of the hyperparameter optimization\n",
        "# df = pd.DataFrame(clf_rf.cv_results_)\n",
        "\n",
        "# # Get the best parameters for the Random Forest estimator\n",
        "# try:\n",
        "#   # Method to avoid overfitting\n",
        "#   min_value = 0.70\n",
        "#   mask = df[\"mean_test_score\"] > min_value\n",
        "#   df_high = df[mask]\n",
        "#   max_value = 0.98\n",
        "#   mask2 = df_high[\"mean_train_score\"] < max_value\n",
        "#   df_high = df_high[mask2]\n",
        "#   df_high[\"test-train\"] = abs(df_high[\"mean_test_score\"]-df_high[\"mean_train_score\"])\n",
        "#   index_min = df_high[\"test-train\"].idxmin(df_high[\"test-train\"])\n",
        "#   best_parameters = df_high.loc[index_min, \"params\"]\n",
        "#   print(f'\\n Using {min_value} as the minimal test accuracy and {max_value} as the maximal training accuracy,'\n",
        "#         f'\\n the best amount of trees is {best_parameters}')\n",
        "#   prevent_overfitting_rf = True\n",
        "\n",
        "# except ValueError:\n",
        "#    # Method if all the training scores are overfit (100%)\n",
        "#   best_parameters = clf_rf.best_params_\n",
        "#   print(f'\\n The classifier is overfit on the training set (train_accuracy > {max_value}) or the test accuracy is always smaller than {min_value}, ' \n",
        "#         f'but the best amount of trees is {best_parameters}')\n",
        "#   prevent_overfitting_rf = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJb6pbVk4Pc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting the Random Forest classifier\n",
        "RF = RandomForestClassifier(n_estimators=20, bootstrap=True)\n",
        "RF.fit(data_train_pca, label_train)\n",
        "\n",
        "# Predictions\n",
        "train_pred_RF = RF.predict(data_train_pca)\n",
        "test_pred_RF = RF.predict(data_test_pca)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAxcrYvhVM1d",
        "colab_type": "code",
        "outputId": "a6d8952d-c133-4f58-c1f0-7c835c67fe60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Evaluate Random Forest using cross-validation\n",
        "score_rf = cross_val_score(RF, data_train_pca, label_train, cv=5)\n",
        "mean_score_rf = score_rf.mean()\n",
        "\n",
        "print(f'\\n The accuracy of the validation set in 5 different folds is {score_rf}')\n",
        "print(f'\\n The mean accuracy of the validation set of 5 different folds is {round(mean_score_rf,2)}')\n",
        "\n",
        "# Evaluate accuracy of Random Forest Classifier on training set\n",
        "acc_train_RF = accuracy_score(label_train, train_pred_RF)\n",
        "print(f'\\n The accuracy on the training set is {round(acc_train_RF, 2)}')\n",
        "\n",
        "# Evaluate accuracy of Random Forest Classifier on test set\n",
        "acc_test_RF = accuracy_score(label_test, test_pred_RF)\n",
        "print(f'\\n The accuracy on the test set is {round(acc_test_RF, 2)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " The accuracy of the validation set in 5 different folds is [0.55555556 0.74074074 0.62962963 0.42307692 0.5       ]\n",
            "\n",
            " The mean accuracy of the validation set of 5 different folds is 0.57\n",
            "\n",
            " The accuracy on the training set is 0.96\n",
            "\n",
            " The accuracy on the test set is 0.59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDVEd0zhh3Vu",
        "colab_type": "code",
        "outputId": "6965cf69-a19f-4497-a496-b3d18e63c4bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "source": [
        "# Confusion matrices and classification reports\n",
        "print('Confusion matrix and classification report of training set')\n",
        "print(confusion_matrix(label_train, train_pred_RF))\n",
        "print(classification_report(label_train, train_pred_RF))\n",
        "print(\"---------------------------------------------------------------\\n\")\n",
        "\n",
        "print('Confusion matrix and classification report of test set')\n",
        "print(confusion_matrix(label_test, test_pred_RF))\n",
        "print(classification_report(label_test, test_pred_RF))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix and classification report of training set\n",
            "[[81  0]\n",
            " [ 5 47]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      1.00      0.97        81\n",
            "           1       1.00      0.90      0.95        52\n",
            "\n",
            "    accuracy                           0.96       133\n",
            "   macro avg       0.97      0.95      0.96       133\n",
            "weighted avg       0.96      0.96      0.96       133\n",
            "\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Confusion matrix and classification report of test set\n",
            "[[13  8]\n",
            " [ 6  7]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.62      0.65        21\n",
            "           1       0.47      0.54      0.50        13\n",
            "\n",
            "    accuracy                           0.59        34\n",
            "   macro avg       0.58      0.58      0.58        34\n",
            "weighted avg       0.60      0.59      0.59        34\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL04AA6EH9_7",
        "colab_type": "text"
      },
      "source": [
        "## Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrU_YknbHARw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameter optimization of SVM\n",
        "\n",
        "# Define parameters that need to be optimized\n",
        "degrees = [1, 3, 5]\n",
        "coef0s = [1, 0.5, 0.01]\n",
        "slacks = [0.1, 1, 10, 100]\n",
        "gammas = [1, 0.1 ,0.01, 0.001]\n",
        "kernels = ['linear', 'rbf', 'poly']\n",
        "\n",
        "# Creating paramater space\n",
        "param_grid = {'degree': degrees,'coef0': coef0s, 'C': slacks, 'gamma': gammas, 'kernel': kernels}\n",
        "\n",
        "# Execute RandomizedSearchCV to find optimal parameters\n",
        "clf_svm = RandomizedSearchCV(SVC(),param_grid,cv=5, return_train_score=True)\n",
        "\n",
        "# Fit the classifier\n",
        "clf_svm.fit(data_train_pca, label_train)\n",
        "\n",
        "# The best parameters\n",
        "print(f'The best parameters are {clf_svm.best_params_}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYQyMXzSXImU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # The dataframe of the hyperparameter optimization\n",
        "# df = pd.DataFrame(clf_svm.cv_results_)\n",
        "\n",
        "# # Get the best parameters for the Random Forest estimator\n",
        "# try:\n",
        "#   # Method to avoid overfitting\n",
        "#   min_value = 0.70\n",
        "#   mask = df[\"mean_test_score\"] > min_value\n",
        "#   df_high = df[mask]\n",
        "#   max_value = 0.98\n",
        "#   mask2 = df_high[\"mean_train_score\"] < max_value\n",
        "#   df_high = df_high[mask2]\n",
        "#   df_high[\"test-train\"] = abs(df_high[\"mean_test_score\"]-df_high[\"mean_train_score\"])\n",
        "#   index_min = df_high[\"test-train\"].idxmin(df_high[\"test-train\"])\n",
        "#   best_parameters = df_high.loc[index_min, \"params\"]\n",
        "#   print(f'\\n Using {min_value} as the minimal test accuracy and {max_value} as the maximal training accuracy,'\n",
        "#         f'\\n the best amount of trees is {best_parameters}')\n",
        "#   prevent_overfitting_svm = True\n",
        "\n",
        "# except ValueError:\n",
        "#    # Method if all the training scores are overfit (100%)\n",
        "#   best_parameters = clf_svm.best_params_\n",
        "#   print(f'\\n The classifier is overfit on the training set (train_accuracy > {max_value}) or the test accuracy is always smaller than {min_value}, ' \n",
        "#         f'but the best parameters are {best_parameters}')\n",
        "#   prevent_overfitting_svm = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0Zzz85DYOKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting the Support Vector Machine classifier\n",
        "best_C = list(best_parameters.items())[0][1]\n",
        "best_coef = list(best_parameters.items())[1][1]\n",
        "best_degree = list(best_parameters.items())[2][1]\n",
        "best_gamma = list(best_parameters.items())[3][1]\n",
        "best_kernel = list(best_parameters.items())[4][1]\n",
        "\n",
        "SVM = SVC(C=best_C, kernel=str(best_kernel), degree=best_degree, gamma=best_gamma, coef0=best_coef)\n",
        "SVM.fit(data_train_pca, label_train)\n",
        "\n",
        "# Predictions\n",
        "train_pred = SVM.predict(data_train_pca)\n",
        "test_pred = SVM.predict(data_test_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WroktlYhUyat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate the SVM classifier using cross-validation\n",
        "score_svm = cross_val_score(SVM, data_train_pca, label_train, cv=5)\n",
        "mean_score_svm = score_svm.mean()\n",
        "\n",
        "print(f'The accuracy of the validation set in 5 different folds is {score_svm}')\n",
        "print(f'\\n The mean accuracy of the validation set of 5 different folds is {round(mean_score_svm,2)}')\n",
        "\n",
        "# Evaluate accuracy of SVM on training set\n",
        "acc_train = accuracy_score(label_train, train_pred)\n",
        "print(f'\\n The accuracy of the training set is {round(acc_train,2)}')\n",
        "\n",
        "# Evaluate accuracy of SVM on test set\n",
        "acc_test = accuracy_score(label_test, test_pred)\n",
        "print(f'\\n The accuracy of the test set is {round(acc_test,2)}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt_WPxIpgcvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Confusion matrix and classification reports\n",
        "print('Confusion matrix and classification report of training set')\n",
        "print(confusion_matrix(label_train, train_pred))\n",
        "print(classification_report(label_train, train_pred))\n",
        "print(\"---------------------------------------------------------------\\n\")\n",
        "\n",
        "print('Confusion matrix and classification report of test set')\n",
        "print(confusion_matrix(label_test, test_pred))\n",
        "print(classification_report(label_test, test_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSEXIhNo9jtU",
        "colab_type": "text"
      },
      "source": [
        "## Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bODUN5a09lfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameter optimization of Neural Network\n",
        "\n",
        "# Define parameter space that needs to be optimized\n",
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,), (7,2), (7,7,7),(9,9,9,9),(50,50,50,50), (50,50,50,50,50)],\n",
        "    'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}\n",
        "    \n",
        "# Execute RandomizedSearchCV to find optimal parameters\n",
        "clf_mlp = RandomizedSearchCV(MLPClassifier(solver='lbfgs', max_iter=5000), parameter_space, cv=5, return_train_score=True)  # Verbose=2 & refit=True uitzoeken\n",
        "\n",
        "# Fit the classifier\n",
        "clf_mlp.fit(data_df_train, label_train)\n",
        "\n",
        "# The best parameters\n",
        "print(f'The best parameters are {clf_mlp.best_params_}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h28fvMaZq54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # The dataframe of the hyperparameter optimization\n",
        "# df = pd.DataFrame(clf_mlp.cv_results_)\n",
        "\n",
        "# # Get the best parameters for the Random Forest estimator\n",
        "# try:\n",
        "#   # Method to avoid overfitting\n",
        "#   min_value = 0.70\n",
        "#   mask = df[\"mean_test_score\"] > min_value\n",
        "#   df_high = df[mask]\n",
        "#   max_value = 0.98\n",
        "#   mask2 = df_high[\"mean_train_score\"] < max_value\n",
        "#   df_high = df_high[mask2]\n",
        "#   df_high[\"test-train\"] = abs(df_high[\"mean_test_score\"]-df_high[\"mean_train_score\"])\n",
        "#   index_min = df_high[\"test-train\"].idxmin(df_high[\"test-train\"])\n",
        "#   best_parameters = df_high.loc[index_min, \"params\"]\n",
        "#   print(f'\\n Using {min_value} as the minimal test accuracy and {max_value} as the maximal training accuracy,'\n",
        "#         f'\\n the best parameters are {best_parameters}')\n",
        "#   prevent_overfitting_mlp = True\n",
        "\n",
        "# except ValueError:\n",
        "#    # Method if all the training scores are overfit (100%)\n",
        "#   best_parameters = clf_mlp.best_params_\n",
        "#   print(f'\\n The classifier is overfit on the training set (train_accuracy > {max_value}) or the test accuracy is always smaller than {min_value}, ' \n",
        "#         f'\\n but the best parameters are {best_parameters}')\n",
        "#   prevent_overfitting_mlp = False\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuBiRAuB96Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting the Neural Network Classifier with optimal hyperparameters\n",
        "best_solver = list(best_parameters.items())[0][1]\n",
        "best_lr = list(best_parameters.items())[1][1]\n",
        "best_hls = list(best_parameters.items())[2][1]\n",
        "best_alpha = list(best_parameters.items())[3][1]\n",
        "best_act = list(best_parameters.items())[4][1]\n",
        "\n",
        "MLP = MLPClassifier(solver=str(best_solver), learning_rate=str(best_lr),hidden_layer_sizes=best_hls, alpha=best_alpha, activation=str(best_act))\n",
        "MLP.fit(data_df_train, label_train)\n",
        "\n",
        "# Predictions\n",
        "train_pred_MLP = MLP.predict(data_df_train)\n",
        "test_pred_MLP = MLP.predict(data_df_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiVEssBr98he",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate neural network using cross-validation\n",
        "score = cross_val_score(MLP, data_df_train, label_train, cv=5)\n",
        "mean_score = score.mean()\n",
        "\n",
        "print(f'\\n The accuracy of the validation set in 5 different folds is {score}')\n",
        "print(f'\\n The mean accuracy of the validation set of 5 different folds is {round(mean_score,2)}')\n",
        "\n",
        "# Evaluate accuracy of neural network on training set\n",
        "acc_train = accuracy_score(label_train, train_pred_MLP)\n",
        "print(f'\\n The accuracy of the training set is {round(acc_train,2)}')\n",
        "\n",
        "# Evaluate accuracy of neural network on test set\n",
        "acc_test = accuracy_score(label_test, test_pred_MLP)\n",
        "print(f'\\n The accuracy of the test set is {round(acc_test,2)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjjObAu39_wV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Confusion matrices and classification reports\n",
        "print('Confusion matrix and classification report of training set')\n",
        "print(confusion_matrix(label_train, train_pred_MLP))\n",
        "print(classification_report(label_train, train_pred_MLP))\n",
        "print(\"---------------------------------------------------------------\\n\")\n",
        "\n",
        "print('Confusion matrix and classification report of test set')\n",
        "print(confusion_matrix(label_test, test_pred_MLP))\n",
        "print(classification_report(label_test, test_pred_MLP))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwGUOWXusaFh",
        "colab_type": "text"
      },
      "source": [
        "# Learning Curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jnh5imnpsYjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function definition of learning curves\n",
        "def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    axes.set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes.set_ylim(*ylim)\n",
        "    axes.set_xlabel(\"Training examples\")\n",
        "    axes.set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores  = \\\n",
        "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes.grid()\n",
        "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes.legend(loc=\"best\")\n",
        "\n",
        "    return plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4djNT-CZAADv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifiers\n",
        "clsfs = [KNN, RF, SVM, MLP]\n",
        "\n",
        "# Plot figuren\n",
        "fig = plt.figure(figsize=(24,4*len(clsfs)))\n",
        "  \n",
        "# Create a cross-validation object\n",
        "cv = ShuffleSplit(n_splits=10, test_size=0.15, random_state=0)\n",
        "\n",
        "# Now use the classifiers on all datasets\n",
        "num = 0\n",
        "for clf in clsfs:\n",
        "    # Split data in training and testing\n",
        "    # title = str(type(clf))\n",
        "    if hasattr(clf, 'n_estimators'):\n",
        "        title = f\"Random Forest Classifier, #trees = {clf.n_estimators}, overfitting prevention = {prevent_overfitting_rf}\"\n",
        "    elif hasattr(clf, 'C'):\n",
        "        title = f\"SVM Classifier, C (slack) = {clf.C}, overfitting prevention = {prevent_overfitting_svm}\"\n",
        "    elif hasattr(clf, 'n_neighbors'):\n",
        "        title = f\"kNN Classifier, #neighbors = {clf.n_neighbors}, overfitting prevention = {prevent_overfitting_knn}\"\n",
        "    elif hasattr(clf, 'hidden_layer_sizes'):\n",
        "        title = f\"Neural Network, overfitting prevention = {prevent_overfitting_mlp}\"\n",
        "    ax = fig.add_subplot(2, 2, num + 1)\n",
        "    plot_learning_curve(clf, title, data_df_train, label_train, ax, ylim=(0.3, 1.01), cv=cv)\n",
        "    num += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
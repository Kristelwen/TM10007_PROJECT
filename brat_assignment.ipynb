{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "brat_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ySDv5ko54ViJ",
        "gY9t_GtJ1KrG",
        "tzrJpQt-PM9u",
        "wO_MGCSW8d0l",
        "FqLl7RNggs9n",
        "gL04AA6EH9_7",
        "fuluNEEqjEoY"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kristelwen/TM10007_PROJECT/blob/master/brat_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment\n",
        "Kristel, Lalot, Marijn, Tahisa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CiDn2Sk-VWqE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "777baa67-82a5-475e-b266-bc5e69e8dab4"
      },
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/Kristelwen/TM10007_PROJECT"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for brats (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySDv5ko54ViJ",
        "colab_type": "text"
      },
      "source": [
        "# Data loading and importing modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_Lae4Zh5V3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing modules\n",
        "# General packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from sklearn import datasets as ds\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Preprocessing packages\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import RFECV\n",
        "\n",
        "# Classifiers\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# SVM Kernels\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.metrics.pairwise import rbf_kernel, sigmoid_kernel\n",
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "# Model selection\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# Performance metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Regularization\n",
        "from sklearn.linear_model import Lasso, RidgeClassifier\n",
        "from sklearn.feature_selection import SelectFromModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-NE_fTbKGe5z",
        "outputId": "c29d0880-bfa3-476e-b9a4-302b8fd482e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Loading the data\n",
        "from brats.load_data import load_data\n",
        "\n",
        "data = load_data()\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of samples: 167\n",
            "The number of columns: 725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50S8Z8Y968kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_top = data.head()  \n",
        "\n",
        "# display  \n",
        "# data_top "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY9t_GtJ1KrG",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl2rwSBvHPjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop columns which contain NaN values\n",
        "threshold = math.floor(len(data)/2)  # calculate threshold, half of total rows\n",
        "data_drop = data.dropna(thresh=threshold, axis=1)  # Delete columns/features with more than 'threshold' NaNs\n",
        "data_drop = data_drop.dropna(axis=0)  # Delete rows/subjects with NaNs\n",
        "\n",
        "# Split data and labels\n",
        "labels = data_drop['label']\n",
        "data_drop = data_drop.drop(columns=\"label\")  # Data without labels\n",
        "\n",
        "# Convert labels 'GBM' and 'LGG' to respectively 0 and 1\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split the data in a train (90%) and test set (10%)\n",
        "data_train, data_test, label_train, label_test = train_test_split(data_drop, labels, test_size=0.1)\n",
        "data_train2, data_val, label_train2, label_val = train_test_split(data_train, label_train, test_size=0.1)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = RobustScaler()\n",
        "transformer = scaler.fit(data_train2.values)  # Fit on training data 2\n",
        "data_scaled_train2 = transformer.transform(data_train2.values)\n",
        "data_df_train2 = pd.DataFrame(data_scaled_train2, index = data_train2.index, columns = data_train2.columns)\n",
        "\n",
        "data_scaled_val = transformer.transform(data_val.values)\n",
        "data_df_val = pd.DataFrame(data_scaled_val, index = data_val.index, columns = data_val.columns)\n",
        "\n",
        "data_scaled_test = transformer.transform(data_test.values)\n",
        "data_df_test = pd.DataFrame(data_scaled_test, index = data_test.index, columns = data_test.columns)\n",
        "\n",
        "scaler2 = RobustScaler()\n",
        "transformer2 = scaler2.fit(data_train.values)  # Fit on training data 1\n",
        "data_scaled_train = transformer2.transform(data_train.values)\n",
        "data_df_train = pd.DataFrame(data_scaled_train, index = data_train.index, columns = data_train.columns)\n",
        "\n",
        "# Feature selection: PCA\n",
        "  # Training set 2\n",
        "pca_train = PCA(n_components=80)  # Create a PCA with 20 components\n",
        "pca_train.fit(data_scaled_train2)  # Fit PCA\n",
        "data_train_pca2 = pca_train.transform(data_scaled_train2)  # Transform train data using PCA\n",
        "#df_train_pca2 = pd.DataFrame(data_train_pca2, index = data_scaled_train2.index)  # Put train data back in dataframe with 20 most important features\n",
        " \n",
        "  # Training set 1\n",
        "data_train_pca = pca_train.transform(data_train)\n",
        "  # Validatie set\n",
        "data_val_pca = pca_train.transform(data_scaled_val)  # Transform test data using PCA\n",
        "  # Test set\n",
        "data_test_pca = pca_train.transform(data_scaled_test)  # Transform test data using PCA\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANFuoo-r_yek",
        "colab_type": "text"
      },
      "source": [
        "# Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzrJpQt-PM9u",
        "colab_type": "text"
      },
      "source": [
        "## Ridgen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G_NXuWnEkgR",
        "colab_type": "code",
        "outputId": "cf27772a-407c-4bd6-b8d5-b0a122b00158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        }
      },
      "source": [
        "# Display the weights and compute error for multiple values for alpha\n",
        "n_alphas = 200\n",
        "alphas = np.logspace(-10, -1, n_alphas)\n",
        "\n",
        "# Construct classifiers\n",
        "coefs = []\n",
        "accuracies = []\n",
        "times = []\n",
        "for a in alphas:\n",
        "    # Fit classifier\n",
        "    clf = RidgeClassifier(alpha=a, fit_intercept=False)\n",
        "    t0 = time()\n",
        "    clf.fit(data_train_pca, label_train)\n",
        "    duration = time() - t0\n",
        "    val_pred = clf.predict(data_val_pca)\n",
        "    message = (\"\\t Misclassified: %d / %d\" % ((label_validation != val_pred).sum(), label_validation.shape[0]))\n",
        "    # print(message)\n",
        "    \n",
        "    # Append statistics\n",
        "    accuracy = float((label_validation != val_pred).sum()) / float(label_validation.shape[0])\n",
        "    times.append(duration)\n",
        "    accuracies.append(accuracy)\n",
        "    coefs.append(clf.coef_)\n",
        "\n",
        "# #############################################################################\n",
        "# Display results\n",
        "\n",
        "# Weights\n",
        "plt.figure()\n",
        "ax = plt.gca()\n",
        "ax.plot(alphas, np.squeeze(coefs))\n",
        "ax.set_xscale('log')\n",
        "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
        "plt.xlabel('alpha')\n",
        "plt.ylabel('weights')\n",
        "plt.title('Ridge coefficients as a function of the regularization')\n",
        "plt.axis('tight')\n",
        "plt.show()\n",
        "\n",
        "# Performance\n",
        "plt.figure()\n",
        "ax = plt.gca()\n",
        "ax.plot(alphas, accuracies)\n",
        "ax.set_xscale('log')\n",
        "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
        "plt.xlabel('alpha')\n",
        "plt.ylabel('accuracies')\n",
        "plt.title('Performance as a function of the regularization')\n",
        "plt.axis('tight')\n",
        "plt.show()\n",
        "\n",
        "# Times\n",
        "plt.figure()\n",
        "ax = plt.gca()\n",
        "ax.plot(alphas, times)\n",
        "ax.set_xscale('log')\n",
        "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
        "plt.xlabel('alpha')\n",
        "plt.ylabel('times (s)')\n",
        "plt.title('Fitting time as a function of the regularization')\n",
        "plt.axis('tight')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEaCAYAAAA/lAFyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmaWiROImZll4gRiZmaZOIGYmVkm/x+Zrm+F4i234wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEaCAYAAAAG87ApAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEaCAYAAAA2f6EIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWWd0nzQPEfa",
        "colab_type": "text"
      },
      "source": [
        "## Random Forest Classifier with PCA\n",
        "Zoals ik het nu heb aangepast kan denk ik blokje 6, 20 en 21 weg. Zijn jullie het hiermee eens? Heb de accuracy op de nette manier berekend ook nu. En random forest niet meer gefit op de validatie set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA1o4T0F6lOy",
        "colab_type": "code",
        "outputId": "7ce22cc1-7c58-4b93-b263-c06e302fef46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "# IK (KRISTEL) DENK DAT DIT HELE BLOKJE WEG KAN, EVEN CHECKEN !! -> CHECKEN VOOR BOOTSTRAP KOMT IN HYPERPARAMETER OPTIMIZATION OOK NAAR VOREN\n",
        "#default settings \n",
        "n_samples=len(data_val_pca)\n",
        "homemade_random_forest = BaggingClassifier(RandomForestClassifier()) \n",
        "homemade_random_forest.fit(data_train_pca,label_train) #fit the classifier on the training set \n",
        "test=homemade_random_forest.predict(data_val_pca) #test \n",
        "error= (sum(abs(test - label_val))/n_samples)*100\n",
        "error = (round(error, 2))\n",
        "print ('the error when using default settings is {}%'.format(error))\n",
        "\n",
        "print ('')\n",
        "\n",
        "#using different n_trees and bootstrapping  \n",
        "n_trees=[5,10,50,100,150,200,300,400,500,600]\n",
        "for n_tree in n_trees:\n",
        "  clf = RandomForestClassifier(n_estimators=n_tree, bootstrap=True)\n",
        "  clf.fit(data_train_pca2,label_train2)\n",
        "  \n",
        "  predictions_train = clf.predict(data_train_pca2)\n",
        "  errors_train = (sum(abs(predictions_train - label_train2))/n_samples)*100\n",
        "  errors_train = (round(errors_train, 2))\n",
        "  \n",
        "  print ('Train set: When using bootstrapping and {} trees, the error is {}%'.format(n_tree,errors_train))\n",
        "\n",
        "  predictions_val = clf.predict(data_val_pca)\n",
        "  errors_val = (sum(abs(predictions_val - label_val))/n_samples)*100\n",
        "  errors_val = (round(errors_val, 2))\n",
        "\n",
        "  print ('Validation set: When using bootstrapping and {} trees, the error is {}%'.format(n_tree,errors_val))\n",
        "\n",
        "print ('')\n",
        "\n",
        "#using different n_trees without bootstrapping  \n",
        "n_trees=[5,10,50,100,150,200,300,400,500,600]\n",
        "for n_tree in n_trees:\n",
        "  clf = RandomForestClassifier(n_estimators=n_tree, bootstrap=False)\n",
        "  clf.fit(data_train_pca2,label_train2)\n",
        "  \n",
        "  predictions_train = clf.predict(data_train_pca2)\n",
        "  errors_train = (sum(abs(predictions_train - label_train2))/n_samples)*100\n",
        "  errors_train = (round(errors_train, 2))\n",
        "  \n",
        "  print ('Train set: Without bootstrapping and {} trees, the error is {}%'.format(n_tree,errors_train))\n",
        "\n",
        "  predictions_val = clf.predict(data_val_pca)\n",
        "  errors_val = (sum(abs(predictions_val - label_val))/n_samples)*100\n",
        "  errors_val = (round(errors_val, 2))\n",
        "\n",
        "  print ('Validation set: Without bootstrapping and {} trees, the error is {}%'.format(n_tree,errors_val))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the error when using default settings is 14.29%\n",
            "\n",
            "Train set: When using bootstrapping and 5 trees, the error is 7.14%\n",
            "Validation set: When using bootstrapping and 5 trees, the error is 35.71%\n",
            "Train set: When using bootstrapping and 10 trees, the error is 21.43%\n",
            "Validation set: When using bootstrapping and 10 trees, the error is 35.71%\n",
            "Train set: When using bootstrapping and 50 trees, the error is 0.0%\n",
            "Validation set: When using bootstrapping and 50 trees, the error is 14.29%\n",
            "Train set: When using bootstrapping and 100 trees, the error is 0.0%\n",
            "Validation set: When using bootstrapping and 100 trees, the error is 21.43%\n",
            "Train set: When using bootstrapping and 150 trees, the error is 0.0%\n",
            "Validation set: When using bootstrapping and 150 trees, the error is 14.29%\n",
            "Train set: When using bootstrapping and 200 trees, the error is 0.0%\n",
            "Validation set: When using bootstrapping and 200 trees, the error is 14.29%\n",
            "Train set: When using bootstrapping and 300 trees, the error is 0.0%\n",
            "Validation set: When using bootstrapping and 300 trees, the error is 21.43%\n",
            "Train set: When using bootstrapping and 400 trees, the error is 0.0%\n",
            "Validation set: When using bootstrapping and 400 trees, the error is 21.43%\n",
            "Train set: When using bootstrapping and 500 trees, the error is 0.0%\n",
            "Validation set: When using bootstrapping and 500 trees, the error is 14.29%\n",
            "Train set: When using bootstrapping and 600 trees, the error is 0.0%\n",
            "Validation set: When using bootstrapping and 600 trees, the error is 21.43%\n",
            "\n",
            "Train set: Without bootstrapping and 5 trees, the error is 0.0%\n",
            "Validation set: Without bootstrapping and 5 trees, the error is 21.43%\n",
            "Train set: Without bootstrapping and 10 trees, the error is 0.0%\n",
            "Validation set: Without bootstrapping and 10 trees, the error is 28.57%\n",
            "Train set: Without bootstrapping and 50 trees, the error is 0.0%\n",
            "Validation set: Without bootstrapping and 50 trees, the error is 21.43%\n",
            "Train set: Without bootstrapping and 100 trees, the error is 0.0%\n",
            "Validation set: Without bootstrapping and 100 trees, the error is 14.29%\n",
            "Train set: Without bootstrapping and 150 trees, the error is 0.0%\n",
            "Validation set: Without bootstrapping and 150 trees, the error is 28.57%\n",
            "Train set: Without bootstrapping and 200 trees, the error is 0.0%\n",
            "Validation set: Without bootstrapping and 200 trees, the error is 21.43%\n",
            "Train set: Without bootstrapping and 300 trees, the error is 0.0%\n",
            "Validation set: Without bootstrapping and 300 trees, the error is 14.29%\n",
            "Train set: Without bootstrapping and 400 trees, the error is 0.0%\n",
            "Validation set: Without bootstrapping and 400 trees, the error is 14.29%\n",
            "Train set: Without bootstrapping and 500 trees, the error is 0.0%\n",
            "Validation set: Without bootstrapping and 500 trees, the error is 14.29%\n",
            "Train set: Without bootstrapping and 600 trees, the error is 0.0%\n",
            "Validation set: Without bootstrapping and 600 trees, the error is 21.43%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w27ZGUSg9TPx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "17bb13cf-12d6-4429-a5e1-c9444c569b69"
      },
      "source": [
        "# Hyperparameter optimization of Random Forest Classifier\n",
        "\n",
        "# Our parameter to optimize is the number of estimators, which we vary uniformly between 1 and 400\n",
        "param_distributions = {'n_estimators': randint(1, 400)}\n",
        "    \n",
        "# Execute RandomizedSearchCV to find optimal parameters\n",
        "clf_rf = RandomizedSearchCV(RandomForestClassifier(), param_distributions, cv=5, n_iter=20, random_state=42)\n",
        "\n",
        "# Fit the classifier\n",
        "clf_rf.fit(data_train_pca2, label_train2)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=5, error_score=nan,\n",
              "                   estimator=RandomForestClassifier(bootstrap=True,\n",
              "                                                    ccp_alpha=0.0,\n",
              "                                                    class_weight=None,\n",
              "                                                    criterion='gini',\n",
              "                                                    max_depth=None,\n",
              "                                                    max_features='auto',\n",
              "                                                    max_leaf_nodes=None,\n",
              "                                                    max_samples=None,\n",
              "                                                    min_impurity_decrease=0.0,\n",
              "                                                    min_impurity_split=None,\n",
              "                                                    min_samples_leaf=1,\n",
              "                                                    min_samples_split=2,\n",
              "                                                    min_weight_fraction_leaf=0.0,\n",
              "                                                    n_estimators=100,\n",
              "                                                    n_jobs=None,\n",
              "                                                    oob_score=False,\n",
              "                                                    random_state=None,\n",
              "                                                    verbose=0,\n",
              "                                                    warm_start=False),\n",
              "                   iid='deprecated', n_iter=20, n_jobs=None,\n",
              "                   param_distributions={'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f4bbd07b9b0>},\n",
              "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGMR4Isu_WjQ",
        "colab_type": "code",
        "outputId": "1527b85f-b528-4363-c04c-6a337f0bed94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Get the best estimator and best parameters belonging to that estimator\n",
        "print(f'\\n The best estimator is {clf_rf.best_estimator_} \\n The best amount of trees is {clf_rf.best_params_}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " The best estimator is RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=103,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False) \n",
            " The best amount of trees is {'n_estimators': 103}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KasLabs11qBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91916da7-9f85-4315-cab2-9d67a8712872"
      },
      "source": [
        "# KAN DENK IK WEG!\n",
        "#Random Forest classifier as a function \n",
        "def RandomForest(n_trees,data_train_pca2,data_val_pca,labels_train2,labels_val):\n",
        "  clf = RandomForestClassifier(n_estimators=n_trees, bootstrap=True)\n",
        "  clf.fit(data_train_pca2,label_train2)\n",
        "  predictions_train = clf.predict(data_train_pca2)\n",
        "  acc_train = accuracy_score(label_train2, predictions_train)\n",
        "\n",
        "  # acc/error op Beun manier WEGHALEN?!\n",
        "  # n_samples=len(data_train_pca2)\n",
        "  # errors_train = (sum(abs(predictions_train - label_train2))/n_samples)*100\n",
        "  # errors_train = (round(errors_train, 2))\n",
        "  # accuracy_train=100-errors_train\n",
        "\n",
        "  # clf.fit(data_val_pca,label_val)  # DEZE REGEL WEGGEHAALD WANT JE MOET NIET FITTEN OP VALIDATIE?\n",
        "  predictions_val = clf.predict(data_val_pca)\n",
        "  acc_val = accuracy_score(label_val, predictions_val)\n",
        "\n",
        "  # Acc/error op Beun manier WEGHALEN?!\n",
        "  # n_samples=len(data_val_pca)\n",
        "  # errors_val = (sum(abs(predictions_val - label_val))/n_samples)*100\n",
        "  # errors_val = (round(errors_val, 2))\n",
        "  # accuracy_val=100-errors_val\n",
        "\n",
        "  return (acc_train, acc_val)\n",
        "\n",
        "# Evaluate accuracy of random forest on training and validation set\n",
        "acc = RandomForest(15,data_train_pca2,data_val_pca,label_train2,label_val)\n",
        "\n",
        "print(f'Accuracy of Random Forest Classifier for the training set is {acc[0]} and for the validation set is {acc[1]}')\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Random Forest Classifier for the training set is 1.0 and for the validation set is 0.7142857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJb6pbVk4Pc1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "fbf1682a-900b-4eb4-bd5f-a78fe42695b1"
      },
      "source": [
        "# DIT STUK IPV DE 2 BLOKJES HIERBOVEN?!\n",
        "# Fitting the Random Forest classifier\n",
        "n_trees = 15\n",
        "RF = RandomForestClassifier(n_estimators=n_trees, bootstrap=True)\n",
        "RF.fit(data_train_pca2, label_train2)\n",
        "\n",
        "# Predictions\n",
        "train2_pred_RF = RF.predict(data_train_pca2)\n",
        "val_pred_RF = RF.predict(data_val_pca)\n",
        "train_pred_RF = RF.predict(data_train_pca)\n",
        "test_pred_RF = RF.predict(data_test_pca)\n",
        "\n",
        "# Evaluate accuracy of Random Forest Classifier on training set\n",
        "acc_train_RF = accuracy_score(label_train2, train2_pred_RF)\n",
        "print(f'\\n The accuracy of the training set is {acc_train_RF}')\n",
        "\n",
        "# Evaluate accuracy of Random Forest Classifier on validation set\n",
        "acc_val_RF = accuracy_score(label_val, val_pred_RF)\n",
        "print(f'\\n The accuracy of the validation set is {acc_val_RF}')\n",
        "\n",
        "# Evaluate accuracy of Random Forest Classifier on test set\n",
        "acc_test_RF = accuracy_score(label_test, test_pred_RF)\n",
        "print(f'\\n The accuracy of the test set is {acc_test_RF}')\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " The accuracy of the training set is 1.0\n",
            "\n",
            " The accuracy of the validation set is 0.8571428571428571\n",
            "\n",
            " The accuracy of the test set is 0.8666666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qs2lcvbEhDk",
        "colab_type": "code",
        "outputId": "9a8372d8-4cc6-49b3-ca7f-a1ebb4c2cd6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# KAN DENK IK WEG?!\n",
        "# Evaluate accuracy of random forest \n",
        "predictions_test = clf.predict(data_test_pca)\n",
        "n_samples=len(data_test_pca)\n",
        "errors_test = (sum(abs(predictions_test - label_test))/n_samples)*100\n",
        "errors_test = (round(errors_test, 2))\n",
        "accuracy_test=100-errors_test\n",
        "\n",
        "print(f'Accuracy of Random Forest Classifier for the test set is {accuracy_test}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Random Forest Classifier for the test set is 86.67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO_MGCSW8d0l",
        "colab_type": "text"
      },
      "source": [
        "## Random forest with feature_importances_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djSbNBU-Dq5v",
        "colab_type": "code",
        "outputId": "a6d562a8-1b9f-49d5-9c15-6340eea4b962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        }
      },
      "source": [
        "# Feature importances\n",
        "forest = RandomForestClassifier(n_estimators=100, bootstrap=True)\n",
        "\n",
        "forest.fit(data_df_train2, label_train2)\n",
        "importances = forest.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
        "             axis=0)\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Print the feature ranking\n",
        "print(\"Feature ranking:\")\n",
        "\n",
        "# for f in range(data_df_train2.shape[1]):\n",
        "    # print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
        "\n",
        "# Plot the feature importances of the forest\n",
        "plt.figure()\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(range(data_df_train2.shape[1]), importances[indices],\n",
        "       color=\"r\", yerr=std[indices], align=\"center\")\n",
        "plt.xticks(range(data_df_train2.shape[1]), indices)\n",
        "plt.xlim([-1, data_df_train2.shape[1]])\n",
        "plt.show()\n",
        "\n",
        "# Only keep features with importance > 0.00\n",
        "mask = importances > 0\n",
        "mask2 = data_df_train2.columns[mask]\n",
        "data_train2_selection = data_df_train2[mask2]\n",
        "print(data_train2_selection)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature ranking:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEICAYAAACuxNj9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "              VOLUME_ET  VOLUME_NET  ...  TGM_Cog_Y_1   TGM_T_1\n",
            "ID                                   ...                       \n",
            "TCGA-76-6664  -0.192073   -0.341534  ...    -1.300943 -0.447847\n",
            "TCGA-HT-7690  -0.007164    4.505023  ...    -0.794279  1.479518\n",
            "TCGA-HT-8111  -0.474489   -0.420584  ...    -0.813479 -0.781609\n",
            "TCGA-02-0085   0.254606   -0.316455  ...    -0.514006 -0.121796\n",
            "TCGA-06-0137   1.021397   -0.172802  ...     0.897209  0.255732\n",
            "...                 ...         ...  ...          ...       ...\n",
            "TCGA-19-5954  -0.080364   -0.217093  ...     0.630053 -0.383578\n",
            "TCGA-02-0047   1.042106    0.000000  ...     0.313475 -0.011195\n",
            "TCGA-14-1794   1.501423    1.470411  ...    -0.813735  0.982596\n",
            "TCGA-06-1084   0.261801   -0.362176  ...    -0.136651  0.014476\n",
            "TCGA-06-0142   1.031908   -0.115458  ...    -0.260002  0.455674\n",
            "\n",
            "[117 rows x 395 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34UwTlXt8iXk",
        "colab_type": "code",
        "outputId": "36dd7602-bec1-4398-e4f1-93f4a3160a57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "#default settings \n",
        "n_samples=len(data_scaled_val)\n",
        "# BaggingClassifier().coef_ = BaggingClassifier().feature_importances_\n",
        "homemade_random_forest = BaggingClassifier(RandomForestClassifier()) \n",
        "# homemade_random_forest.coef_ = homemade_random_forest.feature_importances_\n",
        "selector = RFECV(homemade_random_forest, step=1, cv=5)\n",
        "selector = selector.fit(data_df_train2, label_train2)\n",
        "selector.support_\n",
        "print(selector.ranking_)\n",
        "\n",
        "\n",
        "homemade_random_forest.fit(data_scaled_train2,label_train2) #fit the classifier on the training set \n",
        "test=homemade_random_forest.predict(data_scaled_val) #test \n",
        "error= (sum(abs(test - label_val))/n_samples)*100\n",
        "error = (round(error, 2))\n",
        "print ('the error when using default settings is {}%'.format(error))\n",
        "\n",
        "print ('')#  ff voor overzicht \n",
        "\n",
        "#using different n_trees and bootstrapping  \n",
        "n_trees=[5,10,50,100,150,200,300,400,500,600]\n",
        "for n_tree in n_trees:\n",
        "  clf = RandomForestClassifier(n_estimators=n_tree, bootstrap=True)\n",
        "  clf.fit(data_scaled_train2, label_train2)\n",
        "  \n",
        "  predictions_train = clf.predict(data_scaled_train2)\n",
        "  errors_train = (sum(abs(predictions_train - label_train2))/n_samples)*100\n",
        "  errors_train = (round(errors_train, 2))\n",
        "  \n",
        "  print ('Train set: When using bootstrapping and {} trees, the error is {}%'.format(n_tree,errors_train))\n",
        "\n",
        "  predictions_val = clf.predict(data_scaled_val)\n",
        "  errors_val = (sum(abs(predictions_val - label_val))/n_samples)*100\n",
        "  errors_val = (round(errors_val, 2))\n",
        "\n",
        "  print ('Validation set: When using bootstrapping and {} trees, the error is {}%'.format(n_tree,errors_val))\n",
        "\n",
        "print ('')\n",
        "\n",
        "#using different n_trees without bootstrapping  \n",
        "n_trees=[5,10,50,100,150,200,300,400,500,600]\n",
        "for n_tree in n_trees:\n",
        "  clf = RandomForestClassifier(n_estimators=n_tree, bootstrap=False)\n",
        "  clf.fit(data_scaled_train2,label_train2)\n",
        "  \n",
        "  predictions_train = clf.predict(data_scaled_train2)\n",
        "  errors_train = (sum(abs(predictions_train - label_train2))/n_samples)*100\n",
        "  errors_train = (round(errors_train, 2))\n",
        "  \n",
        "  print ('Train set: Without bootstrapping and {} trees, the error is {}%'.format(n_tree,errors_train))\n",
        "\n",
        "  predictions_val = clf.predict(data_scaled_val)\n",
        "  errors_val = (sum(abs(predictions_val - label_val))/n_samples)*100\n",
        "  errors_val = (round(errors_val, 2))\n",
        "\n",
        "  print ('Validation set: Without bootstrapping and {} trees, the error is {}%'.format(n_tree,errors_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-9beefac36dc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# homemade_random_forest.coef_ = homemade_random_forest.feature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFECV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhomemade_random_forest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mranking_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    531\u001b[0m         scores = parallel(\n\u001b[1;32m    532\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    531\u001b[0m         scores = parallel(\n\u001b[1;32m    532\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36m_rfe_single_fit\u001b[0;34m(rfe, estimator, X, y, train, test, scorer)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     return rfe._fit(\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         _score(estimator, X_test[:, features], y_test, scorer)).scores_\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, step_score)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mcoefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'feature_importances_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcoefs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                 raise RuntimeError('The classifier does not expose '\n\u001b[0m\u001b[1;32m    199\u001b[0m                                    \u001b[0;34m'\"coef_\" or \"feature_importances_\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                                    'attributes')\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The classifier does not expose \"coef_\" or \"feature_importances_\" attributes"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqLl7RNggs9n",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameter optimization of Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Yo1edF-Y_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our parameter to optimize is the number of estimators, which we vary uniformlybetween 1 and 400\n",
        "param_distributions = {'n_estimators': randint(1, 400)}\n",
        "\n",
        "# Now use the classifiers on all datasets\n",
        "fitted_clfs = list()\n",
        "    \n",
        "# Within a 5-fold cross-validation, try out 20 different number of trees\n",
        "clf = RandomizedSearchCV(RandomForestClassifier(), param_distributions, cv=5, n_iter=20, random_state=42)\n",
        "\n",
        "# Fit the classifier\n",
        "clf.fit(data_scaled_train2, label_train2)\n",
        "\n",
        "# Save for next part\n",
        "fitted_clfs.append(clf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txu-c2Eh-dEA",
        "colab_type": "code",
        "outputId": "dcef5ae7-b18c-4f86-9948-ccdf3b424267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Get the best estimator and best parameters belonging to that estimator\n",
        "print(f'\\n The best estimator is {clf.best_estimator_} \\n The best amount of trees is {clf.best_params_}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " The best estimator is RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=271,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False) \n",
            " The best amount of trees is {'n_estimators': 271}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cTNUpu7-h43",
        "colab_type": "code",
        "outputId": "cef7d638-a5b5-4883-897c-37bd16bde1cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Random Forest classifier as a function \n",
        "def RandomForest(n_trees,data_scaled_train,data_scaled_val,labels_train2,labels_val):\n",
        "  clf = RandomForestClassifier(n_estimators=n_trees, bootstrap=True)\n",
        "  clf.fit(data_scaled_train2,label_train2)\n",
        "  predictions_train = clf.predict(data_scaled_train2)\n",
        "  n_samples=len(data_scaled_train2)\n",
        "  errors_train = (sum(abs(predictions_train - label_train2))/n_samples)*100\n",
        "  errors_train = (round(errors_train, 2))\n",
        "  accuracy_train=100-errors_train\n",
        "\n",
        "  clf.fit(data_scaled_val,label_val)\n",
        "  predictions_val = clf.predict(data_scaled_val)\n",
        "  n_samples=len(data_scaled_val)\n",
        "  errors_val = (sum(abs(predictions_val - label_val))/n_samples)*100\n",
        "  errors_val = (round(errors_val, 2))\n",
        "  accuracy_val=100-errors_val\n",
        "\n",
        "  return (accuracy_train, accuracy_val)\n",
        "\n",
        "acc = RandomForest(15,data_scaled_train2,data_scaled_val,label_train2,label_val)\n",
        "print(f'Accuracy of Random Forest Classifier for the training set is {acc[0]} and for the validation set is {acc[1]}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Random Forest Classifier for the training set is 99.15 and for the validation set is 100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usv5Ku1M-ktj",
        "colab_type": "code",
        "outputId": "0ded1b1d-6fb8-4c79-f0da-202e5240da4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predictions_test = clf.predict(data_scaled_test)\n",
        "n_samples=len(data_scaled_test)\n",
        "errors_test = (sum(abs(predictions_test - label_test))/n_samples)*100\n",
        "errors_test = (round(errors_test, 2))\n",
        "accuracy_test=100-errors_test\n",
        "\n",
        "print(f'Accuracy of Random Forest Classifier for the test set is {accuracy_test}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Random Forest Classifier for the test set is 86.67\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL04AA6EH9_7",
        "colab_type": "text"
      },
      "source": [
        "## Support Vector Machine with PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ePcN2CQIBwu",
        "colab_type": "code",
        "outputId": "99783902-f017-47dd-84d4-3d85b7110e10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Construct classifiers\n",
        "svmlin = SVC(kernel='linear', gamma='scale') # linear kernel \n",
        "svmrbf = SVC(kernel='rbf', gamma='scale') # radial basis function kernel \n",
        "svmpoly = SVC(kernel='poly', degree=3, gamma='scale') # polynomial kernel\n",
        "\n",
        "clsfs = [svmlin, svmrbf, svmpoly]\n",
        "\n",
        "# for clf in clsfs: # voor alle classifiers in lijst\n",
        "#    clf.fit(data_train_pca2, label_train2) # fit classifier op trainingsset\n",
        "#    val = clf.predict(data_val) # test classifier op testset\n",
        "#    error = (sum(abs(val - label_val))/len(data_val))*100 # hoevaak fout? \n",
        "#    error = (round(error, 2))\n",
        "#    accuracy = 100 - error\n",
        "    # print(f'The Accuracy of SVM is {accuracy}%')\n",
        "\n",
        "# functie\n",
        "def SVM(clsfs, data_df_train2, data_df_val, label_train2, label_val): \n",
        "    accuracies_train = list()\n",
        "    accuracies_val = list()\n",
        "\n",
        "    for clf in clsfs:\n",
        "      clf.fit(data_df_train2,label_train2)\n",
        "      predictions_train = clf.predict(data_df_train2)\n",
        "      n_samples=len(data_df_train2)\n",
        "      errors_train = (sum(abs(predictions_train - label_train2))/n_samples)*100\n",
        "      errors_train = (round(errors_train, 2))\n",
        "      accuracy_train=100-errors_train\n",
        "      accuracies_train.append(accuracy_train)\n",
        "      \n",
        "      clf.fit(data_df_val,label_val)\n",
        "      predictions_val = clf.predict(data_df_val)\n",
        "      n_samples=len(data_df_val)\n",
        "      errors_val = (sum(abs(predictions_val - label_val))/n_samples)*100\n",
        "      errors_val = (round(errors_val, 2))\n",
        "      accuracy_val=100-errors_val\n",
        "      accuracies_val.append(accuracy_val)\n",
        "\n",
        "    return (accuracies_train, accuracies_val)\n",
        "\n",
        "accuracies_train, accuracies_val = SVM(clsfs, data_train_pca2, data_val_pca, label_train2, label_val)\n",
        "\n",
        "clsfs_names = ['SVM with linear kernel', 'SVM with radial basis function kernel', 'SVM with polynomial kernel']\n",
        "for num, acc_train in enumerate(accuracies_train):\n",
        "    print(f'\\n The accuracy of {clsfs_names[num]} is {acc_train}% for the train set and {accuracies_val[num]}% for the validation set')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " The accuracy of SVM with linear kernel is 100.0% for the train set and 100.0% for the validation set\n",
            "\n",
            " The accuracy of SVM with radial basis function kernel is 67.52000000000001% for the train set and 92.86% for the validation set\n",
            "\n",
            " The accuracy of SVM with polynomial kernel is 67.52000000000001% for the train set and 92.86% for the validation set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuluNEEqjEoY",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrU_YknbHARw",
        "colab_type": "code",
        "outputId": "af2319a1-f2ca-4510-fa04-cf8e9ebee74b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Parameters\n",
        "degrees = randint(1, 5)\n",
        "coef0s = [1, 0.5, 0.01]\n",
        "slacks = [0.1,1, 10, 100]\n",
        "gammas = [1, 0.1 ,0.01, 0.001]\n",
        "kernels = ['linear', 'rbf', 'poly']\n",
        "\n",
        "# Creating paramater space\n",
        "param_grid = {'degree': degrees,'coef0': coef0s, 'C': slacks, 'gamma': gammas, 'kernel': kernels}\n",
        "\n",
        "# Searching for best parameters\n",
        "SVM = RandomizedSearchCV(SVC(),param_grid,cv=5, random_state=42, return_train_score=True)\n",
        "# Fitting classifier on training set\n",
        "SVM.fit(data_train_pca2, label_train2)\n",
        "\n",
        "# Predict parameters\n",
        "train2_pred = SVM.predict(data_train_pca2)\n",
        "val_pred = SVM.predict(data_val_pca)\n",
        "# train_pred = SVM.predict(data_train_pca)\n",
        "test_pred = SVM.predict(data_test_pca)\n",
        "\n",
        "# Calculating scores\n",
        "folds = 3\n",
        "score = cross_val_score(SVM.best_estimator_, data_df_train, label_train, cv=folds)  # Train set\n",
        "mean_score = score.mean()\n",
        "\n",
        "# Print final best estimator and accuracies\n",
        "# Best estimator\n",
        "print(\"\\n###############################################################\")\n",
        "print(f'\\n The best estimator is {SVM.best_estimator_} \\n \\n The best settings are {SVM.best_params_}\\n')\n",
        "print(\"###############################################################\\n\")\n",
        "\n",
        "\n",
        "# Confusion matrix and classification report\n",
        "print('Confusion matrix and classification report of training set')\n",
        "print(confusion_matrix(label_train2, train2_pred))\n",
        "print(classification_report(label_train2, train2_pred))\n",
        "print(\"---------------------------------------------------------------\\n\")\n",
        "\n",
        "print('Confusion matrix and classification report of validation set')\n",
        "print(confusion_matrix(label_val, val_pred))\n",
        "print(classification_report(label_val, val_pred))\n",
        "print(\"---------------------------------------------------------------\\n\")\n",
        "\n",
        "print('Confusion matrix and classification report of test set')\n",
        "print(confusion_matrix(label_test, test_pred))\n",
        "print(classification_report(label_test, test_pred))\n",
        "\n",
        "# Accuracy of Validation set:\n",
        "print(\"\\n###############################################################\\n\")\n",
        "print(f'\\n The accuracy of the validation set in the n={folds} different folds is {score}')\n",
        "print(f'\\n The mean accuracy of the validation set of n={folds} different folds is {mean_score}')\n",
        "print(\"\\n###############################################################\\n\")\n",
        "\n",
        "# Evaluate SVM on training set\n",
        "acc_train = accuracy_score(label_train2, train2_pred)\n",
        "print(f'\\n The accuracy of the training set is {acc_train}')\n",
        "\n",
        "# Evaluate SVM on validation set\n",
        "acc_val = accuracy_score(label_val, val_pred)\n",
        "print(f'\\n The accuracy of the validation set is {acc_val}')\n",
        "\n",
        "# Evaluate SVM on test set\n",
        "acc_test = accuracy_score(label_test, test_pred)\n",
        "print(f'\\n The accuracy of the test set is {acc_test}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "###############################################################\n",
            "\n",
            " The best estimator is SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=1,\n",
            "    decision_function_shape='ovr', degree=3, gamma=0.01, kernel='linear',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False) \n",
            " \n",
            " The best settings are {'C': 10, 'coef0': 1, 'degree': 3, 'gamma': 0.01, 'kernel': 'linear'}\n",
            "\n",
            "###############################################################\n",
            "\n",
            "Confusion matrix and classification report of training set\n",
            "[[83  0]\n",
            " [ 0 34]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        83\n",
            "           1       1.00      1.00      1.00        34\n",
            "\n",
            "    accuracy                           1.00       117\n",
            "   macro avg       1.00      1.00      1.00       117\n",
            "weighted avg       1.00      1.00      1.00       117\n",
            "\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Confusion matrix and classification report of validation set\n",
            "[[10  0]\n",
            " [ 1  3]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95        10\n",
            "           1       1.00      0.75      0.86         4\n",
            "\n",
            "    accuracy                           0.93        14\n",
            "   macro avg       0.95      0.88      0.90        14\n",
            "weighted avg       0.94      0.93      0.93        14\n",
            "\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Confusion matrix and classification report of test set\n",
            "[[8 1]\n",
            " [1 5]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89         9\n",
            "           1       0.83      0.83      0.83         6\n",
            "\n",
            "    accuracy                           0.87        15\n",
            "   macro avg       0.86      0.86      0.86        15\n",
            "weighted avg       0.87      0.87      0.87        15\n",
            "\n",
            "\n",
            "###############################################################\n",
            "\n",
            "\n",
            " The accuracy of the validation set in the n=3 different folds is [0.84090909 0.88636364 0.8372093 ]\n",
            "\n",
            " The mean accuracy of the validation set of n=3 different folds is 0.8548273431994362\n",
            "\n",
            "###############################################################\n",
            "\n",
            "\n",
            " The accuracy of the training set is 1.0\n",
            "\n",
            " The accuracy of the validation set is 0.9285714285714286\n",
            "\n",
            " The accuracy of the test set is 0.8666666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSEXIhNo9jtU",
        "colab_type": "text"
      },
      "source": [
        "## Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bODUN5a09lfX",
        "colab_type": "code",
        "outputId": "f0a39e17-b35d-4b50-cddf-e77180603cf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "# Hyperparameter optimization of Neural Network\n",
        "\n",
        "# Define parameter space that needs to be optimized\n",
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,), (7,2), (7,7,7),(9,9,9,9),(50,50,50,50), (50,50,50,50,50)],\n",
        "    'activation': ['logistic','identity','tanh'], \n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}\n",
        "    \n",
        "# Execute RandomizedSearchCV to find optimal parameters\n",
        "clf_mlp = RandomizedSearchCV(MLPClassifier(max_iter=2000), parameter_space, cv=5, random_state=42, return_train_score=True)  # Verbose=2 & refit=True uitzoeken\n",
        "\n",
        "# Fit the classifier\n",
        "clf_mlp.fit(data_train, label_train)\n",
        "\n",
        "# Get the best parameters for the MLP estimator\n",
        "print('Best parameters found:\\n', clf_mlp.best_params_)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py:151: RuntimeWarning: overflow encountered in matmul\n",
            "  ret = a @ b\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py:151: RuntimeWarning: invalid value encountered in matmul\n",
            "  ret = a @ b\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:768: RuntimeWarning: invalid value encountered in greater\n",
            "  y = np.array(y > threshold, dtype=np.int)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:768: RuntimeWarning: invalid value encountered in greater\n",
            "  y = np.array(y > threshold, dtype=np.int)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py:151: RuntimeWarning: overflow encountered in matmul\n",
            "  ret = a @ b\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py:151: RuntimeWarning: invalid value encountered in matmul\n",
            "  ret = a @ b\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:768: RuntimeWarning: invalid value encountered in greater\n",
            "  y = np.array(y > threshold, dtype=np.int)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:768: RuntimeWarning: invalid value encountered in greater\n",
            "  y = np.array(y > threshold, dtype=np.int)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py:151: RuntimeWarning: overflow encountered in matmul\n",
            "  ret = a @ b\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py:151: RuntimeWarning: invalid value encountered in matmul\n",
            "  ret = a @ b\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:768: RuntimeWarning: invalid value encountered in greater\n",
            "  y = np.array(y > threshold, dtype=np.int)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:768: RuntimeWarning: invalid value encountered in greater\n",
            "  y = np.array(y > threshold, dtype=np.int)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py:151: RuntimeWarning: overflow encountered in matmul\n",
            "  ret = a @ b\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py:151: RuntimeWarning: invalid value encountered in matmul\n",
            "  ret = a @ b\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:768: RuntimeWarning: invalid value encountered in greater\n",
            "  y = np.array(y > threshold, dtype=np.int)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:768: RuntimeWarning: invalid value encountered in greater\n",
            "  y = np.array(y > threshold, dtype=np.int)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py:151: RuntimeWarning: overflow encountered in matmul\n",
            "  ret = a @ b\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py:151: RuntimeWarning: invalid value encountered in matmul\n",
            "  ret = a @ b\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:151: RuntimeWarning: invalid value encountered in reduce\n",
            "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:768: RuntimeWarning: invalid value encountered in greater\n",
            "  y = np.array(y > threshold, dtype=np.int)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:768: RuntimeWarning: invalid value encountered in greater\n",
            "  y = np.array(y > threshold, dtype=np.int)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters found:\n",
            " {'solver': 'adam', 'learning_rate': 'constant', 'hidden_layer_sizes': (7, 7, 7), 'alpha': 0.05, 'activation': 'identity'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBP3UFzJ94oY",
        "colab_type": "code",
        "outputId": "179931c5-58b9-4a7a-b5ea-4782b51b0fab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        }
      },
      "source": [
        "# Plot the dataframe of the hyperparameter optimization\n",
        "pd.DataFrame(clf_mlp.cv_results_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_solver</th>\n",
              "      <th>param_learning_rate</th>\n",
              "      <th>param_hidden_layer_sizes</th>\n",
              "      <th>param_alpha</th>\n",
              "      <th>param_activation</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>split0_train_score</th>\n",
              "      <th>split1_train_score</th>\n",
              "      <th>split2_train_score</th>\n",
              "      <th>split3_train_score</th>\n",
              "      <th>split4_train_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "      <th>std_train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.299022</td>\n",
              "      <td>0.271891</td>\n",
              "      <td>0.004074</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>adam</td>\n",
              "      <td>constant</td>\n",
              "      <td>(7, 2)</td>\n",
              "      <td>0.05</td>\n",
              "      <td>logistic</td>\n",
              "      <td>{'solver': 'adam', 'learning_rate': 'constant'...</td>\n",
              "      <td>0.703704</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.617664</td>\n",
              "      <td>0.155049</td>\n",
              "      <td>9</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.304762</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.616557</td>\n",
              "      <td>0.155902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.478899</td>\n",
              "      <td>1.135125</td>\n",
              "      <td>0.004381</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>sgd</td>\n",
              "      <td>constant</td>\n",
              "      <td>(100,)</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>tanh</td>\n",
              "      <td>{'solver': 'sgd', 'learning_rate': 'constant',...</td>\n",
              "      <td>0.740741</td>\n",
              "      <td>0.730769</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.740456</td>\n",
              "      <td>0.028524</td>\n",
              "      <td>2</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.828571</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.847619</td>\n",
              "      <td>0.876190</td>\n",
              "      <td>0.824524</td>\n",
              "      <td>0.067055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.187029</td>\n",
              "      <td>0.024433</td>\n",
              "      <td>0.004357</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>sgd</td>\n",
              "      <td>constant</td>\n",
              "      <td>(7, 2)</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>identity</td>\n",
              "      <td>{'solver': 'sgd', 'learning_rate': 'constant',...</td>\n",
              "      <td>0.703704</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.694587</td>\n",
              "      <td>0.004558</td>\n",
              "      <td>3</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.694652</td>\n",
              "      <td>0.001172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.109056</td>\n",
              "      <td>0.051416</td>\n",
              "      <td>0.004101</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>adam</td>\n",
              "      <td>adaptive</td>\n",
              "      <td>(7, 2)</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>tanh</td>\n",
              "      <td>{'solver': 'adam', 'learning_rate': 'adaptive'...</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.192308</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.443590</td>\n",
              "      <td>0.208561</td>\n",
              "      <td>10</td>\n",
              "      <td>0.490385</td>\n",
              "      <td>0.380952</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.742857</td>\n",
              "      <td>0.555220</td>\n",
              "      <td>0.139447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.718686</td>\n",
              "      <td>0.424521</td>\n",
              "      <td>0.004168</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>adam</td>\n",
              "      <td>constant</td>\n",
              "      <td>(7, 7, 7)</td>\n",
              "      <td>0.05</td>\n",
              "      <td>identity</td>\n",
              "      <td>{'solver': 'adam', 'learning_rate': 'constant'...</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.730769</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.770940</td>\n",
              "      <td>0.024564</td>\n",
              "      <td>1</td>\n",
              "      <td>0.884615</td>\n",
              "      <td>0.923810</td>\n",
              "      <td>0.942857</td>\n",
              "      <td>0.942857</td>\n",
              "      <td>0.971429</td>\n",
              "      <td>0.933114</td>\n",
              "      <td>0.028624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.111279</td>\n",
              "      <td>0.041251</td>\n",
              "      <td>0.004158</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>adam</td>\n",
              "      <td>constant</td>\n",
              "      <td>(7, 7, 7)</td>\n",
              "      <td>0.05</td>\n",
              "      <td>tanh</td>\n",
              "      <td>{'solver': 'adam', 'learning_rate': 'constant'...</td>\n",
              "      <td>0.740741</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.653846</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.694302</td>\n",
              "      <td>0.027587</td>\n",
              "      <td>7</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.704762</td>\n",
              "      <td>0.684982</td>\n",
              "      <td>0.035508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.219620</td>\n",
              "      <td>0.142995</td>\n",
              "      <td>0.004013</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>adam</td>\n",
              "      <td>adaptive</td>\n",
              "      <td>(7, 2)</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>logistic</td>\n",
              "      <td>{'solver': 'adam', 'learning_rate': 'adaptive'...</td>\n",
              "      <td>0.703704</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.694587</td>\n",
              "      <td>0.004558</td>\n",
              "      <td>3</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.694652</td>\n",
              "      <td>0.001172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.185261</td>\n",
              "      <td>0.109540</td>\n",
              "      <td>0.004246</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>sgd</td>\n",
              "      <td>adaptive</td>\n",
              "      <td>(7, 7, 7)</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>logistic</td>\n",
              "      <td>{'solver': 'sgd', 'learning_rate': 'adaptive',...</td>\n",
              "      <td>0.703704</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.694587</td>\n",
              "      <td>0.004558</td>\n",
              "      <td>3</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.694652</td>\n",
              "      <td>0.001172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.220436</td>\n",
              "      <td>0.104478</td>\n",
              "      <td>0.004046</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>sgd</td>\n",
              "      <td>constant</td>\n",
              "      <td>(7, 7, 7)</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>logistic</td>\n",
              "      <td>{'solver': 'sgd', 'learning_rate': 'constant',...</td>\n",
              "      <td>0.703704</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.694587</td>\n",
              "      <td>0.004558</td>\n",
              "      <td>3</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.694652</td>\n",
              "      <td>0.001172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.354048</td>\n",
              "      <td>0.037892</td>\n",
              "      <td>0.004274</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>adam</td>\n",
              "      <td>constant</td>\n",
              "      <td>(100,)</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>logistic</td>\n",
              "      <td>{'solver': 'adam', 'learning_rate': 'constant'...</td>\n",
              "      <td>0.629630</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.664387</td>\n",
              "      <td>0.034491</td>\n",
              "      <td>8</td>\n",
              "      <td>0.730769</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.695238</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.704249</td>\n",
              "      <td>0.016186</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_fit_time  std_fit_time  ...  mean_train_score  std_train_score\n",
              "0       0.299022      0.271891  ...          0.616557         0.155902\n",
              "1       2.478899      1.135125  ...          0.824524         0.067055\n",
              "2       2.187029      0.024433  ...          0.694652         0.001172\n",
              "3       0.109056      0.051416  ...          0.555220         0.139447\n",
              "4       1.718686      0.424521  ...          0.933114         0.028624\n",
              "5       0.111279      0.041251  ...          0.684982         0.035508\n",
              "6       0.219620      0.142995  ...          0.694652         0.001172\n",
              "7       0.185261      0.109540  ...          0.694652         0.001172\n",
              "8       0.220436      0.104478  ...          0.694652         0.001172\n",
              "9       0.354048      0.037892  ...          0.704249         0.016186\n",
              "\n",
              "[10 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuBiRAuB96Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting the Neural Network Classifier with optimal hyperparameters\n",
        "MLP = MLPClassifier(solver='adam', learning_rate='constant',hidden_layer_sizes=(9,9,9,9), alpha=0.0001, activation='identity')\n",
        "MLP.fit(data_df_train2, label_train2)\n",
        "\n",
        "# Predictions\n",
        "train2_pred_MLP = MLP.predict(data_df_train2)\n",
        "val_pred_MLP = MLP.predict(data_df_val)\n",
        "train_pred_MLP = MLP.predict(data_df_train)\n",
        "test_pred_MLP = MLP.predict(data_df_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiVEssBr98he",
        "colab_type": "code",
        "outputId": "07b20750-7376-4837-e36f-d50d7ae6a764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Evaluate neural network using cross-validation\n",
        "score = cross_val_score(MLP, data_df_train, label_train, cv=10)\n",
        "mean_score = score.mean()\n",
        "\n",
        "print(f'\\n The accuracy of the validation set in 10 different folds is {score}')\n",
        "print(f'\\n The mean accuracy of the validation set of 10 different folds is {mean_score}')\n",
        "\n",
        "# Evaluate accuracy of neural network on training set\n",
        "acc_train = accuracy_score(label_train2, train2_pred_MLP)\n",
        "print(f'\\n The accuracy of the training set is {acc_train}')\n",
        "\n",
        "# Evaluate accuracy of neural network on validation set\n",
        "acc_val = accuracy_score(label_val, val_pred_MLP)\n",
        "print(f'\\n The accuracy of the validation set is {acc_val}')\n",
        "\n",
        "# Evaluate accuracy of neural network on test set\n",
        "acc_test = accuracy_score(label_test, test_pred_MLP)\n",
        "print(f'\\n The accuracy of the test set is {acc_test}')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " The accuracy of the validation set in 10 different folds is [0.78571429 0.76923077 0.69230769 0.92307692 0.84615385 0.69230769\n",
            " 0.92307692 0.76923077 1.         0.92307692]\n",
            "\n",
            " The mean accuracy of the validation set of 10 different folds is 0.8324175824175825\n",
            "\n",
            " The accuracy of the training set is 1.0\n",
            "\n",
            " The accuracy of the validation set is 0.9285714285714286\n",
            "\n",
            " The accuracy of the test set is 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjjObAu39_wV",
        "colab_type": "code",
        "outputId": "e3c6b6c9-8372-4214-f3db-80f67ddb8967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "# Confusion matrices and classification reports\n",
        "print('Confusion matrix and classification report of validation set')\n",
        "print(confusion_matrix(label_val, val_pred))\n",
        "print(classification_report(label_val, val_pred))\n",
        "\n",
        "print('Confusion matrix and classification report of training set')\n",
        "print(confusion_matrix(label_train2, train2_pred))\n",
        "print(classification_report(label_train2, train2_pred))\n",
        "\n",
        "print('Confusion matrix and classification report of test set')\n",
        "print(confusion_matrix(label_test, test_pred))\n",
        "print(classification_report(label_test, test_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix and classification report of validation set\n",
            "[[8 0]\n",
            " [3 3]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      1.00      0.84         8\n",
            "           1       1.00      0.50      0.67         6\n",
            "\n",
            "    accuracy                           0.79        14\n",
            "   macro avg       0.86      0.75      0.75        14\n",
            "weighted avg       0.84      0.79      0.77        14\n",
            "\n",
            "Confusion matrix and classification report of training set\n",
            "[[83  0]\n",
            " [ 0 34]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        83\n",
            "           1       1.00      1.00      1.00        34\n",
            "\n",
            "    accuracy                           1.00       117\n",
            "   macro avg       1.00      1.00      1.00       117\n",
            "weighted avg       1.00      1.00      1.00       117\n",
            "\n",
            "Confusion matrix and classification report of test set\n",
            "[[11  0]\n",
            " [ 2  2]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      1.00      0.92        11\n",
            "           1       1.00      0.50      0.67         4\n",
            "\n",
            "    accuracy                           0.87        15\n",
            "   macro avg       0.92      0.75      0.79        15\n",
            "weighted avg       0.89      0.87      0.85        15\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwGUOWXusaFh",
        "colab_type": "text"
      },
      "source": [
        "# Learning Curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jnh5imnpsYjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function definition of learning curves\n",
        "def plot_learning_curve(estimator, title, X, y, axes, ylim=None, cv=None,\n",
        "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    axes.set_title(title)\n",
        "    if ylim is not None:\n",
        "        axes.set_ylim(*ylim)\n",
        "    axes.set_xlabel(\"Training examples\")\n",
        "    axes.set_ylabel(\"Score\")\n",
        "\n",
        "    train_sizes, train_scores, test_scores  = \\\n",
        "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
        "                       train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    # Plot learning curve\n",
        "    axes.grid()\n",
        "    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                         color=\"r\")\n",
        "    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
        "                         color=\"g\")\n",
        "    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "                 label=\"Training score\")\n",
        "    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "                 label=\"Cross-validation score\")\n",
        "    axes.legend(loc=\"best\")\n",
        "\n",
        "    return plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4djNT-CZAADv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifiers\n",
        "clsfs = [MLP]\n",
        "\n",
        "# Plot figuren\n",
        "fig = plt.figure(figsize=(24,8*len(clsfs)))\n",
        "  \n",
        "# Create a cross-validation object\n",
        "cv = ShuffleSplit(n_splits=10, test_size=0.15, random_state=0)\n",
        "# cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "# cv = StratifiedShuffleSplit(n_splits=10, test_size=0.15, random_state=0)\n",
        "\n",
        "# Now use the classifiers on all datasets\n",
        "num = 0\n",
        "for clf in clsfs:\n",
        "    # Split data in training and testing\n",
        "    # title = str(type(clf))\n",
        "    if hasattr(clf, 'n_estimators'):\n",
        "        title = f\"Random Forest Classifier, n(trees) = {clf.n_estimators}\"\n",
        "    elif hasattr(clf, 'C'):\n",
        "        title = f\"SVM Classifier, C (slack) = {clf.C}\"\n",
        "    elif hasattr(clf, 'n_neighbors'):\n",
        "        title = f\"kNN Classifier, #neighbors = {clf.n_neighbors}\"\n",
        "    elif hasattr(clf, 'hidden_layer_sizes'):\n",
        "        title = f\"Neural Network\"\n",
        "    ax = fig.add_subplot(len(clsfs), 3, num + 1)\n",
        "    plot_learning_curve(clf, title, data_df_train, label_train, ax, ylim=(0.3, 1.01), cv=cv)\n",
        "    num += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYs9pVAD_fjs",
        "colab_type": "text"
      },
      "source": [
        "## Learning Curves oud\n",
        "stond eerst onder function definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE2bTNXT_ZIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OUD VAN LEARNING CURVES, MAG WEG ALS ALLES WERKT\n",
        "# First make plot without classifiers:\n",
        "# Construct classifiers\n",
        "svmlin = SVC(C=2, kernel='linear', gamma='scale') # linear kernel \n",
        "svmrbf = SVC(C=2, kernel='rbf', gamma='scale') # radial basis function kernel \n",
        "svmpoly = SVC(C=2, kernel='poly', degree=3, gamma='scale') # polynomial kernel\n",
        "\n",
        "clsfs = [\n",
        "         RandomForestClassifier(n_estimators=5, min_samples_leaf=3, max_features='auto', random_state=0, bootstrap=True),\n",
        "         RandomForestClassifier(n_estimators=20, min_samples_leaf=3, random_state=0, bootstrap=True),\n",
        "         RandomForestClassifier(n_estimators=50, min_samples_leaf=3, random_state=0, bootstrap=True),\n",
        "         svmlin, svmrbf, svmpoly,\n",
        "         KNeighborsClassifier(n_neighbors=1, weights='distance'),\n",
        "         KNeighborsClassifier(n_neighbors=7, weights='distance'),\n",
        "         KNeighborsClassifier(n_neighbors=10, weights='distance')\n",
        "        ]\n",
        "\n",
        "# Plot figuren\n",
        "fig = plt.figure(figsize=(24,8*len(clsfs)))\n",
        "# ax.scatter(data_train_pca[:, 0], data_train_pca[:, 1], marker='o', c=label_train,\n",
        "#     s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
        "    \n",
        "        \n",
        "# Create a cross-validation object\n",
        "cv = ShuffleSplit(n_splits=10, test_size=0.15, random_state=0)\n",
        "# cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "# cv = StratifiedShuffleSplit(n_splits=10, test_size=0.15, random_state=0)\n",
        "\n",
        "# Now use the classifiers on all datasets\n",
        "num = 0\n",
        "for clf in clsfs:\n",
        "    # Split data in training and testing\n",
        "    # title = str(type(clf))\n",
        "    if hasattr(clf, 'n_estimators'):\n",
        "        title = f\"Random Forest Classifier, n(trees) = {clf.n_estimators}\"\n",
        "    elif hasattr(clf, 'C'):\n",
        "        title = f\"SVM Classifier, C (slack) = {clf.C}\"\n",
        "    elif hasattr(clf, 'n_neighbors'):\n",
        "        title = f\"kNN Classifier, #neighbors = {clf.n_neighbors}\"\n",
        "    ax = fig.add_subplot(len(clsfs), 3, num + 1)\n",
        "    plot_learning_curve(clf, title, data_train_pca, label_train, ax, ylim=(0.3, 1.01), cv=cv)\n",
        "    num += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}